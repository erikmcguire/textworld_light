{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erikmcguire/textworld_light/blob/main/TextWorld_LIGHT_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHurRjJ_AyME"
      },
      "source": [
        "### Dependencies and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR966zEb2r0u"
      },
      "source": [
        "##### Installs - Restart after running (before imports)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulUkyn6miYxt"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "# Ubuntu no longer distributes chromium-browser outside of snap\n",
        "#\n",
        "# Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap\n",
        "\n",
        "# Add debian buster\n",
        "cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n",
        "EOF\n",
        "\n",
        "# Add keys\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
        "\n",
        "apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n",
        "apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n",
        "apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n",
        "\n",
        "# Prefer debian repo for chromium* packages only\n",
        "# Note the double-blank lines between entries\n",
        "cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n",
        "Package: *\n",
        "Pin: release a=eoan\n",
        "Pin-Priority: 500\n",
        "\n",
        "\n",
        "Package: *\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 300\n",
        "\n",
        "\n",
        "Package: chromium*\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 700\n",
        "EOF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ1G61kkiecC"
      },
      "outputs": [],
      "source": [
        "!apt-get update &> /dev/null\n",
        "!apt-get install chromium chromium-driver &> /dev/null\n",
        "!pip3 install selenium &> /dev/null\n",
        "!pip install urllib3 &> /dev/null\n",
        "!pip install textworld textworld[vis] &> /dev/null\n",
        "!pip3 install deepsig &> /dev/null\n",
        "!pip3 install scipy==1.10.0 &> /dev/null\n",
        "!pip install pingouin pyyaml==5.4.1 &> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj91zCc52pyq"
      },
      "source": [
        "##### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0XegcLMSbm2"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(category=UserWarning,\n",
        "                                            action='ignore')\n",
        "warnings.filterwarnings(category=DeprecationWarning,\n",
        "                                            action='ignore')\n",
        "import pandas as pd\n",
        "import glob\n",
        "from os.path import join as pjoin\n",
        "from collections import OrderedDict, defaultdict, Counter\n",
        "import textworld\n",
        "import locale\n",
        "from textworld import GameMaker, g_rng\n",
        "\n",
        "from textworld.generator.data import KnowledgeBase\n",
        "from textworld.logic import GameLogic\n",
        "from textworld.generator.game import GameOptions, GrammarOptions\n",
        "from textworld.generator.text_grammar import Grammar\n",
        "from textworld.generator.text_grammar import Grammar as GrammarO\n",
        "from textworld.envs.wrappers import Recorder\n",
        "from textworld.generator.game import Game, World, Quest, Event, EntityInfo\n",
        "\n",
        "import textworld.gym\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "import gym\n",
        "from more_itertools import powerset\n",
        "\n",
        "from time import time\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interactive, interactive_output, fixed, interact_manual\n",
        "from IPython.display import display\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "from deepsig import aso\n",
        "\n",
        "from google.colab import runtime\n",
        "\n",
        "\n",
        "import scipy\n",
        "scipy.__version__\n",
        "import spacy\n",
        "from scipy import stats\n",
        "from scipy.stats import shapiro, kstest\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UigO619C_408"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7edgezj3IxO"
      },
      "outputs": [],
      "source": [
        "DATA_PTH = \"/../content/drive/MyDrive/data/light_data/\"\n",
        "dfqq_quest_global_graph = pd.read_pickle(f\"{DATA_PTH}dfqq_quest_global_graph.pkl\")\n",
        "vod_df_qs = pd.read_pickle(f\"{DATA_PTH}vod_df_qs.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAoK_dIGHcwr"
      },
      "source": [
        "### Parse objects, affordances, create data files used above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sDFEOsNyt27"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import random\n",
        "import locale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q45oE-Kf9ePz"
      },
      "outputs": [],
      "source": [
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34S0KrzdKzM1"
      },
      "outputs": [],
      "source": [
        "!pip install spacy-transformers &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BFcADszSjHJ"
      },
      "outputs": [],
      "source": [
        "!python3 -m spacy download en_core_web_trf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTqU8CihVAif"
      },
      "outputs": [],
      "source": [
        "import spacy_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHcsjkPr_Mly"
      },
      "outputs": [],
      "source": [
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load('en_core_web_trf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdzCbESnUa_V",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Utils\n",
        "\n",
        "vset = {'drink','drop','eat','follow','get','give',\n",
        "        'go','hit','hug','put','remove','steal',\n",
        "        'use','wear','wield'}\n",
        "\n",
        "def get_object(doc):\n",
        "    # https://subscription.packtpub.com/book/data/9781838987312/2/ch02lvl1sec16/extracting-subjects-and-objects-of-the-sentence\n",
        "    for token in doc:\n",
        "        if (\"dobj\" in token.dep_):\n",
        "            subtree = list(token.subtree)\n",
        "            start = subtree[0].i\n",
        "            end = subtree[-1].i + 1\n",
        "            return doc[start:end]\n",
        "\n",
        "def get_o(vo, doc):\n",
        "    # The 15 Light commands\n",
        "    vset = ['drink','drop','eat','follow','get','give',\n",
        "            'go','hit','hug','put','remove','steal',\n",
        "            'use','wear','wield']\n",
        "    try:\n",
        "        nc = next(doc.noun_chunks)\n",
        "    except:\n",
        "        pass\n",
        "    o = None\n",
        "    for token in doc:\n",
        "        if str(token) in vset and str(token.dep_) != \"ROOT\":\n",
        "            # probably needs det if first verb not root\n",
        "            vo2 = vo.split()[0] + \" a \" + \" \".join(vo.split()[1:])\n",
        "            doc = nlp(vo2)\n",
        "            for token in doc:\n",
        "                # try def vs indef articles if still does't work\n",
        "                if str(token) in vset and str(token.dep_) != \"ROOT\":\n",
        "                    vo3 = vo.split()[0] + \" the \" + \" \".join(vo.split()[1:])\n",
        "                    doc = nlp(vo3)\n",
        "    for token in doc:\n",
        "        if str(token) in vset and str(token.dep_) != \"ROOT\":\n",
        "            # give up and use latter part of original action\n",
        "            o = \" \".join(vo.split()[1:])\n",
        "        elif str(token) in vset and str(token.dep_) == \"ROOT\":\n",
        "            o = get_object(doc)\n",
        "            o = str(o)\n",
        "            o = o.replace(\"the \", \"\").replace(\"a \", \"\")\n",
        "            try:\n",
        "                nctr = nc.text.replace(\"the \", \"\").replace(\"a \", \"\")\n",
        "                if nctr.split(\" \")[0] in vset:\n",
        "                    nctr = \" \".join(nctr.split(\" \")[1:])\n",
        "                if o not in nctr:\n",
        "                    o = nctr\n",
        "            except:\n",
        "                pass\n",
        "    if o == \"None\":\n",
        "        o = \" \".join(vo.split()[1:])\n",
        "    return o\n",
        "\n",
        "vset = {'drink','drop','eat','follow','get','give',\n",
        "            'go','hit','hug','put','remove','steal',\n",
        "            'use','wear','wield'}\n",
        "\n",
        "def get_vod_qs(df, ky = \"questl\"):\n",
        "    \"\"\"Create counts of commands (action-object pairings) per gender,\n",
        "    e.g. male: wield+sword (vod_df[\"M\"][\"wield\"][\"sword\"])\n",
        "    occurs 58 times\n",
        "    \"\"\"\n",
        "    vod = {g: {v: dict()\n",
        "            for v in vset}\n",
        "       for g in [\"M\", \"F\", \"N\"]}\n",
        "    # Count\n",
        "    for g in [\"M\", \"F\", \"N\"]:\n",
        "        vo_pairs_g = df[df.gender == g]\n",
        "        for ix in vo_pairs_g.index:\n",
        "            vol = vo_pairs_g[ky][ix]\n",
        "            for vo in vol:\n",
        "                v = vo.split()[0]\n",
        "                doc = nlp(vo)\n",
        "                o = get_o(vo, doc)\n",
        "                if not o.strip():\n",
        "                    o = \" \".join(vo.split()[1:])\n",
        "                if not o in vod[g][v].keys():\n",
        "                    vod[g][v][o] = [ix]\n",
        "                else:\n",
        "                    vod[g][v][o].append(ix)\n",
        "    return vod\n",
        "\n",
        "def get_simplified_qs(vvod_df):\n",
        "    vod_df = vvod_df.copy(deep=True)\n",
        "    for g in [\"M\", \"F\", \"N\"]:\n",
        "        objects = set() # create initial list of single word objects\n",
        "        for v, od in vod_df[g].items():\n",
        "            for o, qixs in od.items():\n",
        "                if len(o.split(\" \")) == 1:\n",
        "                    objects.add(o)\n",
        "        # replace instances where duplicates hidden by modifiers\n",
        "        # e.g. as in multiword yet contains object above\n",
        "        # ex: 'hot tea' can be seen as dupe of 'tea'\n",
        "        for v, od in vod_df[g].items():\n",
        "            new_od = copy.deepcopy(od) # can't change od size during loop\n",
        "            for o, qixs in od.items():\n",
        "                maybe_add = []\n",
        "                if len(o.split(\" \")) > 1: # ex: 'hot tea'\n",
        "                    old_o = o\n",
        "                    o = o.split(\" because \")[0]\n",
        "                    o = o.split(\" and \")[0]\n",
        "                    o = o.split(\" from \")[0]\n",
        "                    o = o.split(\" to \")[0]\n",
        "                    o = o.split(\" with \")[0]\n",
        "                    o = o.split(\" in \")[0]\n",
        "                    o = o.split(\" inside \")[0]\n",
        "                    o = o.split(\" into \")[0]\n",
        "                    o = o.split(\" on \")[0]\n",
        "                    o = o.split(\" onto \")[0]\n",
        "                    for el in o.split(\" \"): # ex: ['hot', 'tea']\n",
        "                        if el in objects: # e.g. 'tea' in objects\n",
        "                            # Assume match can be replaced\n",
        "                            # e.g. 'hot tea' effectively replaced by 'tea'\n",
        "                            # by extending 'tea' quest indices and removing 'hot tea'\n",
        "                            maybe_add.append((el, qixs))\n",
        "                    if len(maybe_add) > 1:\n",
        "                        el, qixs = maybe_add[-1] # could go by more common, but use final, assume prev are modifiers\n",
        "                    if not el in new_od.keys():\n",
        "                        new_od[el] = [qixs]\n",
        "                    else:\n",
        "                        new_od[el].extend(qixs) # increment simple\n",
        "                    if old_o in new_od.keys():\n",
        "                        new_od.pop(old_o) # remove multiword\n",
        "            vod_df[g][v] = new_od\n",
        "    vod = vod_df.to_dict()\n",
        "    return vod_df, vod\n",
        "\n",
        "def get_ix2v_global(df):\n",
        "    ix2v = {ix: {\"pairs\": dict()} for ix in dfqq.index}\n",
        "    o2v = dict()\n",
        "    for g in [\"M\", \"F\", \"N\"]:\n",
        "        o2v[g] = dict()\n",
        "        for v, od in vod_df_qs[g].items():\n",
        "            for o, _ in od.items():\n",
        "                if o not in o2v[g].keys():\n",
        "                    o2v[g][o] = {v}\n",
        "                else:\n",
        "                    o2v[g][o].add(v)\n",
        "    for g in [\"M\", \"F\", \"N\"]:\n",
        "        for v, od in vod_df_qs[g].items():\n",
        "            for o, qixs in od.items():\n",
        "                for ix in qixs:\n",
        "                    if type(ix) == int:\n",
        "                        if o not in ix2v[ix][\"pairs\"].keys():\n",
        "                            ix2v[ix][\"pairs\"][o] = o2v[g][o]\n",
        "                    elif type(ix) == list:\n",
        "                        while type(ix) == list:\n",
        "                            ix = ix[0]\n",
        "                            if o not in ix2v[ix][\"pairs\"].keys():\n",
        "                                ix2v[ix][\"pairs\"][o] = o2v[g][o]\n",
        "    return ix2v\n",
        "\n",
        "def get_ix2v(vod_df_qs):\n",
        "    ix2v = {ix: {\"pairs\": dict()} for ix in dfqq.index}\n",
        "    for g in [\"M\", \"F\", \"N\"]:\n",
        "        for v, od in vod_df_qs[g].items():\n",
        "            for o, qixs in od.items():\n",
        "                for ix in qixs:\n",
        "                    if type(ix) == int:\n",
        "                        if o not in ix2v[ix][\"pairs\"].keys():\n",
        "                            ix2v[ix][\"pairs\"][o] = {v}\n",
        "                        else:\n",
        "                            ix2v[ix][\"pairs\"][o].add(v)\n",
        "                    elif type(ix) == list:\n",
        "                        while type(ix) == list:\n",
        "                            ix = ix[0]\n",
        "                            if o not in ix2v[ix][\"pairs\"].keys():\n",
        "                                ix2v[ix][\"pairs\"][o] = {v}\n",
        "                            else:\n",
        "                                ix2v[ix][\"pairs\"][o].add(v)\n",
        "    return ix2v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjpxdUV2lmsW"
      },
      "outputs": [],
      "source": [
        "#vod_df_qs = pd.DataFrame.from_dict(get_vod_qs(dfqq, \"questl\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPVBvW6GwoUI"
      },
      "outputs": [],
      "source": [
        "# ix2v_df = pd.DataFrame.from_dict(get_ix2v(vod_df_qs))\n",
        "# ix2v_df = ix2v_df.T\n",
        "ix2v_df = pd.read_pickle(f\"{DATA_PTH}ix2v.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeJhgn3rFWpG"
      },
      "outputs": [],
      "source": [
        "ix2v_df_global = pd.DataFrame.from_dict(get_ix2v_global(vod_df_qs))\n",
        "ix2v_df_global = ix2v_df_global.T.rename({\"pairs\": \"pairs_global\"}, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ej5dyt3o06d"
      },
      "outputs": [],
      "source": [
        "#vod_df_qss, vod_qs = get_simplified_qs(vod_df_qs)\n",
        "vod_df_qss = pd.read_pickle(f\"{DATA_PTH}vod_df_qss.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.read_pickle(f\"{DATA_PTH}merged_dfo.pkl\") # json vers of graph"
      ],
      "metadata": {
        "id": "RKqR4_kEbzrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa9Lsa9R7Na2"
      },
      "outputs": [],
      "source": [
        "dfqq_global = dfqq.join(ix2v_df_global)\n",
        "dfqq_global_graph = dfqq_global.merge(merged_df, how=\"left\", on=\"quest\", suffixes=(None, \"_y\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P1z9mWLAfVT"
      },
      "outputs": [],
      "source": [
        "# dfqq_quest_based = dfqq.join(ix2v_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfqq = pd.read_pickle(f\"{DATA_PTH}dfqq_gmn.pkl\")"
      ],
      "metadata": {
        "id": "OsRU7Zm_b5NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N2FNqMoAn1U"
      },
      "outputs": [],
      "source": [
        "dfqq_quest_global = dfqq.join(ix2v_df).join(ix2v_df_global)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSXCUSY4A6-t"
      },
      "outputs": [],
      "source": [
        "dfqq_quest_global_graph = dfqq_quest_global.merge(merged_df, how=\"left\", on=\"quest\", suffixes=(None, \"_y\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJLwVri33-GW"
      },
      "outputs": [],
      "source": [
        "# ix2v_df.T.to_pickle(f\"{DATA_PTH}ix2v.pkl\")\n",
        "# ix2v_df_global.to_pickle(f\"{DATA_PTH}ix2v_global.pkl\")\n",
        "# vod_df_qs.to_pickle(f\"{DATA_PTH}vod_df_qs.pkl\")\n",
        "# vod_df_qss.to_pickle(f\"{DATA_PTH}vod_df_qss.pkl\")\n",
        "# dfqq_quest_based.to_pickle(f\"{DATA_PTH}dfqq_quest_based.pkl\")\n",
        "# dfqq_quest_global.to_pickle(f\"{DATA_PTH}dfqq_quest_global.pkl\")\n",
        "# dfqq_global.to_pickle(f\"{DATA_PTH}dfqq_global.pkl\")\n",
        "# dfqq_global_graph.to_pickle(f\"{DATA_PTH}dfqq_global_graph.pkl\")\n",
        "# dfqq_quest_global_graph.to_pickle(f\"{DATA_PTH}dfqq_quest_global_graph.pkl\")\n",
        "# vod_df_qss.to_pickle(f\"{DATA_PTH}vod_df_qs_simplified.pkl\") # after running fn to replace multiword on loaded vod_df.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwQKjkxGB7Sc"
      },
      "source": [
        "### Logic, grammar for game creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ko_TRJDi-Oa"
      },
      "source": [
        "#### Set custom logic (broken across cells due to length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9njlxk6AXGnV"
      },
      "outputs": [],
      "source": [
        "#@title twl 1\n",
        "\n",
        "twl = '''\n",
        "# container\n",
        "type c : t {\n",
        "    predicates {\n",
        "        open(c);\n",
        "        closed(c);\n",
        "        locked(c);\n",
        "\n",
        "        in(o, c);\n",
        "    }\n",
        "\n",
        "    rules {\n",
        "        lock/c   :: $at(P, r) & $at(c, r) & $in(k, I) & $match(k, c) & closed(c) -> locked(c);\n",
        "        unlock/c :: $at(P, r) & $at(c, r) & $in(k, I) & $match(k, c) & locked(c) -> closed(c);\n",
        "\n",
        "        open/c  :: $at(P, r) & $at(c, r) & closed(c) -> open(c);\n",
        "        close/c :: $at(P, r) & $at(c, r) & open(c) -> closed(c);\n",
        "    }\n",
        "\n",
        "    reverse_rules {\n",
        "        lock/c :: unlock/c;\n",
        "        open/c :: close/c;\n",
        "    }\n",
        "\n",
        "    constraints {\n",
        "        c1 :: open(c)   & closed(c) -> fail();\n",
        "        c2 :: open(c)   & locked(c) -> fail();\n",
        "        c3 :: closed(c) & locked(c) -> fail();\n",
        "    }\n",
        "\n",
        "    inform7 {\n",
        "        type {\n",
        "            kind :: \"container\";\n",
        "            definition :: \"containers are openable, lockable and fixed in place. containers are usually closed.\";\n",
        "        }\n",
        "\n",
        "        predicates {\n",
        "            open(c) :: \"The {c} is open\";\n",
        "            closed(c) :: \"The {c} is closed\";\n",
        "            locked(c) :: \"The {c} is locked\";\n",
        "\n",
        "            in(o, c) :: \"The {o} is in the {c}\";\n",
        "        }\n",
        "\n",
        "        commands {\n",
        "            open/c :: \"open {c}\" :: \"opening the {c}\";\n",
        "            close/c :: \"close {c}\" :: \"closing the {c}\";\n",
        "\n",
        "            lock/c :: \"lock {c} with {k}\" :: \"locking the {c} with the {k}\";\n",
        "            unlock/c :: \"unlock {c} with {k}\" :: \"unlocking the {c} with the {k}\";\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Inventory\n",
        "type I {\n",
        "    predicates {\n",
        "        in(o, I);\n",
        "    }\n",
        "\n",
        "    rules {\n",
        "        inventory :: at(P, r) -> at(P, r);  # Nothing changes.\n",
        "\n",
        "        take :: $at(P, r) & at(o, r) -> in(o, I);\n",
        "        drop :: $at(P, r) & in(o, I) -> at(o, r);\n",
        "\n",
        "        take/c :: $at(P, r) & $at(c, r) & $open(c) & in(o, c) -> in(o, I);\n",
        "        insert :: $at(P, r) & $at(c, r) & $open(c) & in(o, I) -> in(o, c);\n",
        "\n",
        "        take/s :: $at(P, r) & $at(s, r) & on(o, s) -> in(o, I);\n",
        "        put    :: $at(P, r) & $at(s, r) & in(o, I) -> on(o, s);\n",
        "\n",
        "        examine/I :: in(o, I) -> in(o, I);  # Nothing changes.\n",
        "        examine/s :: at(P, r) & $at(s, r) & $on(o, s) -> at(P, r);  # Nothing changes.\n",
        "        examine/c :: at(P, r) & $at(c, r) & $open(c) & $in(o, c) -> at(P, r);  # Nothing changes.\n",
        "    }\n",
        "\n",
        "    reverse_rules {\n",
        "        inventory :: inventory;\n",
        "\n",
        "        take :: drop;\n",
        "        take/c :: insert;\n",
        "        take/s :: put;\n",
        "\n",
        "        examine/I :: examine/I;\n",
        "        examine/s :: examine/s;\n",
        "        examine/c :: examine/c;\n",
        "    }\n",
        "\n",
        "    inform7 {\n",
        "        predicates {\n",
        "            in(o, I) :: \"The player carries the {o}\";\n",
        "        }\n",
        "\n",
        "        commands {\n",
        "            take :: \"take {o}\" :: \"taking the {o}\";\n",
        "            drop :: \"drop {o}\" :: \"dropping the {o}\";\n",
        "\n",
        "            take/c :: \"take {o} from {c}\" :: \"removing the {o} from the {c}\";\n",
        "            insert :: \"insert {o} into {c}\" :: \"inserting the {o} into the {c}\";\n",
        "\n",
        "            take/s :: \"take {o} from {s}\" :: \"removing the {o} from the {s}\";\n",
        "            put :: \"put {o} on {s}\" :: \"putting the {o} on the {s}\";\n",
        "\n",
        "            inventory :: \"inventory\" :: \"taking inventory\";\n",
        "\n",
        "            examine/I :: \"examine {o}\" :: \"examining the {o}\";\n",
        "            examine/s :: \"examine {o}\" :: \"examining the {o}\";\n",
        "            examine/c :: \"examine {o}\" :: \"examining the {o}\";\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# supporter\n",
        "type s : t {\n",
        "    predicates {\n",
        "        on(o, s);\n",
        "    }\n",
        "\n",
        "    inform7 {\n",
        "        type {\n",
        "            kind :: \"supporter\";\n",
        "            definition :: \"supporters are fixed in place.\";\n",
        "        }\n",
        "\n",
        "        predicates {\n",
        "            on(o, s) :: \"The {o} is on the {s}\";\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# door\n",
        "type d : t {\n",
        "    predicates {\n",
        "        open(d);\n",
        "        closed(d);\n",
        "        locked(d);\n",
        "\n",
        "        link(r, d, r);\n",
        "    }\n",
        "\n",
        "    rules {\n",
        "        lock/d   :: $at(P, r) & $link(r, d, r') & $link(r', d, r) & $in(k, I) & $match(k, d) & closed(d) -> locked(d);\n",
        "        unlock/d :: $at(P, r) & $link(r, d, r') & $link(r', d, r) & $in(k, I) & $match(k, d) & locked(d) -> closed(d);\n",
        "\n",
        "        open/d   :: $at(P, r) & $link(r, d, r') & $link(r', d, r) & closed(d) -> open(d) & free(r, r') & free(r', r);\n",
        "        close/d  :: $at(P, r) & $link(r, d, r') & $link(r', d, r) & open(d) & free(r, r') & free(r', r) -> closed(d);\n",
        "\n",
        "        examine/d :: at(P, r) & $link(r, d, r') -> at(P, r);  # Nothing changes.\n",
        "    }\n",
        "\n",
        "    reverse_rules {\n",
        "        lock/d :: unlock/d;\n",
        "        open/d :: close/d;\n",
        "\n",
        "        examine/d :: examine/d;\n",
        "    }\n",
        "\n",
        "    constraints {\n",
        "        d1 :: open(d)   & closed(d) -> fail();\n",
        "        d2 :: open(d)   & locked(d) -> fail();\n",
        "        d3 :: closed(d) & locked(d) -> fail();\n",
        "\n",
        "        # A door can't be used to link more than two rooms.\n",
        "        link1 :: link(r, d, r') & link(r, d, r'') -> fail();\n",
        "        link2 :: link(r, d, r') & link(r'', d, r''\\') -> fail();\n",
        "\n",
        "        # There's already a door linking two rooms.\n",
        "        link3 :: link(r, d, r') & link(r, d', r') -> fail();\n",
        "\n",
        "        # There cannot be more than four doors in a room.\n",
        "        too_many_doors :: link(r, d1: d, r1: r) & link(r, d2: d, r2: r) & link(r, d3: d, r3: r) & link(r, d4: d, r4: r) & link(r, d5: d, r5: r) -> fail();\n",
        "\n",
        "        # There cannot be more than four doors in a room.\n",
        "        dr1 :: free(r, r1: r) & link(r, d2: d, r2: r) & link(r, d3: d, r3: r) & link(r, d4: d, r4: r) & link(r, d5: d, r5: r) -> fail();\n",
        "        dr2 :: free(r, r1: r) & free(r, r2: r) & link(r, d3: d, r3: r) & link(r, d4: d, r4: r) & link(r, d5: d, r5: r) -> fail();\n",
        "        dr3 :: free(r, r1: r) & free(r, r2: r) & free(r, r3: r) & link(r, d4: d, r4: r) & link(r, d5: d, r5: r) -> fail();\n",
        "        dr4 :: free(r, r1: r) & free(r, r2: r) & free(r, r3: r) & free(r, r4: r) & link(r, d5: d, r5: r) -> fail();\n",
        "\n",
        "        free1 :: link(r, d, r') & free(r, r') & closed(d) -> fail();\n",
        "        free2 :: link(r, d, r') & free(r, r') & locked(d) -> fail();\n",
        "    }\n",
        "\n",
        "    inform7 {\n",
        "        type {\n",
        "            kind :: \"door\";\n",
        "            definition :: \"door is openable and lockable.\";\n",
        "        }\n",
        "\n",
        "        predicates {\n",
        "            open(d) :: \"The {d} is open\";\n",
        "            closed(d) :: \"The {d} is closed\";\n",
        "            locked(d) :: \"The {d} is locked\";\n",
        "            link(r, d, r') :: \"\";  # No equivalent in Inform7.\n",
        "        }\n",
        "\n",
        "        commands {\n",
        "            open/d :: \"open {d}\" :: \"opening {d}\";\n",
        "            close/d :: \"close {d}\" :: \"closing {d}\";\n",
        "\n",
        "            unlock/d :: \"unlock {d} with {k}\" :: \"unlocking {d} with the {k}\";\n",
        "            lock/d :: \"lock {d} with {k}\" :: \"locking {d} with the {k}\";\n",
        "\n",
        "            examine/d :: \"examine {d}\" :: \"examining {d}\";\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# object\n",
        "type o : t {\n",
        "    predicates {\n",
        "        wearable(o);\n",
        "        wieldable(o);\n",
        "        huggable(o);\n",
        "        hittable(o);\n",
        "        followable(o);\n",
        "        worn(o);\n",
        "        wielded(o);\n",
        "    }\n",
        "    constraints {\n",
        "        obj1 :: in(o, I) & in(o, c) -> fail();\n",
        "        obj2 :: in(o, I) & on(o, s) -> fail();\n",
        "        obj3 :: in(o, I) & at(o, r) -> fail();\n",
        "        obj4 :: in(o, c) & on(o, s) -> fail();\n",
        "        obj5 :: in(o, c) & at(o, r) -> fail();\n",
        "        obj6 :: on(o, s) & at(o, r) -> fail();\n",
        "        obj7 :: at(o, r) & at(o, r') -> fail();\n",
        "        obj8 :: in(o, c) & in(o, c') -> fail();\n",
        "        obj9 :: on(o, s) & on(o, s') -> fail();\n",
        "    }\n",
        "\n",
        "    rules {\n",
        "        wear :: $in(o, I) & $wearable(o) -> worn(o);\n",
        "        wield :: $in(o, I) & $wieldable(o) -> wielded(o);\n",
        "        hit :: at(P, r) & $hittable(o) & $at(o, r) -> at(P, r);\n",
        "        hug :: at(P, r) & $huggable(o) & $at(o, r) -> at(P, r);\n",
        "        follow :: at(P, r) & $followable(o) & $at(o, r) -> at(P, r);\n",
        "        use :: in(o, I) -> in(o, I);\n",
        "    }\n",
        "\n",
        "\n",
        "    inform7 {\n",
        "        type {\n",
        "            kind :: \"object-like\";\n",
        "            definition :: \"object-like is portable. object-like can be huggable. object-like can be hittable. object-like can be followable. object-like can be wearable. object-like can be wieldable. object-like can be wielded.\";\n",
        "        }\n",
        "        commands {\n",
        "            wear :: \"wear {o}\" :: \"You put on the {o}\";\n",
        "            wield :: \"wield {o}\" :: \"You swing the {o} around a few times.\";\n",
        "            hit :: \"hit {o}\" :: \"You hit the {o}.\";\n",
        "            hug :: \"hug {o}\" :: \"You hug the {o}.\";\n",
        "            use :: \"use {o}\" :: \"You use the {o}.\";\n",
        "            follow :: \"follow {o}\" :: \"You follow the {o}.\";\n",
        "        }\n",
        "\n",
        "        predicates {\n",
        "            wearable(o) :: \"The {o} is wearable\";\n",
        "            wieldable(o) :: \"The {o} is wieldable\";\n",
        "            worn(o) :: \"The {o} is worn\";\n",
        "            wielded(o) :: \"The {o} is wielded\";\n",
        "            huggable(o) :: \"The {o} is huggable\";\n",
        "            hittable(o) :: \"The {o} is hittable\";\n",
        "            followable(o) :: \"The {o} is followable\";\n",
        "        }\n",
        "\n",
        "        code :: \"\"\"\n",
        "            Understand \"steal [something]\" as taking.\n",
        "            Understand \"hug [something]\" as kissing.\n",
        "            Understand \"give [something] to [something]\" as giving it to.\n",
        "            Understand \"give [something] [something]\" as giving it to (with nouns reversed).\n",
        "            Understand \"drop [something worn]\" as taking off.\n",
        "            Wielding is an action applying to one carried thing.\n",
        "            Understand \"wield [something]\" as wielding.\n",
        "            Check an actor wielding:\n",
        "                if the noun is not wieldable:\n",
        "                    say \"[The noun] is not a weapon!\";\n",
        "                    stop the action.\n",
        "            After wielding the noun:\n",
        "                now the noun is wielded;\n",
        "                say \"You swing [the noun] around a few times.\";\n",
        "\n",
        "            Following is an action applying to one thing.\n",
        "            Understand \"follow [something]\" as following.\n",
        "            Check an actor following:\n",
        "                if the noun is not followable:\n",
        "                    say \"[The noun] can not be followed!\";\n",
        "                    stop the action.\n",
        "            After following the noun:\n",
        "                say \"You follow [the noun].\";\n",
        "\n",
        "            Using is an action applying to one thing.\n",
        "            Understand \"use [something]\" as using.\n",
        "            After using the noun:\n",
        "                say \"You use [the noun].\";\n",
        "\n",
        "        \"\"\";\n",
        "    }\n",
        "}\n",
        "\n",
        "# Player\n",
        "type P {\n",
        "    rules {\n",
        "        look :: at(P, r) -> at(P, r);  # Nothing changes.\n",
        "    }\n",
        "\n",
        "    reverse_rules {\n",
        "        look :: look;\n",
        "    }\n",
        "\n",
        "    inform7 {\n",
        "        commands {\n",
        "            look :: \"look\" :: \"looking\";\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# room\n",
        "type r {\n",
        "    predicates {\n",
        "        at(P, r);\n",
        "        at(t, r);\n",
        "\n",
        "        north_of(r, r);\n",
        "        west_of(r, r);\n",
        "\n",
        "        north_of/d(r, d, r);\n",
        "        west_of/d(r, d, r);\n",
        "\n",
        "        free(r, r);\n",
        "\n",
        "        south_of(r, r') = north_of(r', r);\n",
        "        east_of(r, r') = west_of(r', r);\n",
        "\n",
        "        south_of/d(r, d, r') = north_of/d(r', d, r);\n",
        "        east_of/d(r, d, r') = west_of/d(r', d, r);\n",
        "    }\n",
        "\n",
        "    rules {\n",
        "        go/north :: at(P, r) & $north_of(r', r) & $free(r, r') & $free(r', r) -> at(P, r');\n",
        "        go/south :: at(P, r) & $south_of(r', r) & $free(r, r') & $free(r', r) -> at(P, r');\n",
        "        go/east  :: at(P, r) & $east_of(r', r) & $free(r, r') & $free(r', r) -> at(P, r');\n",
        "        go/west  :: at(P, r) & $west_of(r', r) & $free(r, r') & $free(r', r) -> at(P, r');\n",
        "    }\n",
        "\n",
        "    reverse_rules {\n",
        "        go/north :: go/south;\n",
        "        go/west :: go/east;\n",
        "    }\n",
        "\n",
        "    constraints {\n",
        "        r1 :: at(P, r) & at(P, r') -> fail();\n",
        "        r2 :: at(s, r) & at(s, r') -> fail();\n",
        "        r3 :: at(c, r) & at(c, r') -> fail();\n",
        "\n",
        "        # An exit direction can only lead to one room.\n",
        "        nav_rr1 :: north_of(r, r') & north_of(r'', r') -> fail();\n",
        "        nav_rr2 :: south_of(r, r') & south_of(r'', r') -> fail();\n",
        "        nav_rr3 :: east_of(r, r') & east_of(r'', r') -> fail();\n",
        "        nav_rr4 :: west_of(r, r') & west_of(r'', r') -> fail();\n",
        "\n",
        "        # Two rooms can only be connected once with each other.\n",
        "        nav_rrA :: north_of(r, r') & south_of(r, r') -> fail();\n",
        "        nav_rrB :: north_of(r, r') & west_of(r, r') -> fail();\n",
        "        nav_rrC :: north_of(r, r') & east_of(r, r') -> fail();\n",
        "        nav_rrD :: south_of(r, r') & west_of(r, r') -> fail();\n",
        "        nav_rrE :: south_of(r, r') & east_of(r, r') -> fail();\n",
        "        nav_rrF :: west_of(r, r')  & east_of(r, r') -> fail();\n",
        "    }\n",
        "\n",
        "    inform7 {\n",
        "        type {\n",
        "            kind :: \"room\";\n",
        "        }\n",
        "\n",
        "        predicates {\n",
        "            at(P, r) :: \"The player is in {r}\";\n",
        "            at(t, r) :: \"The {t} is in {r}\";\n",
        "            free(r, r') :: \"\";  # No equivalent in Inform7.\n",
        "\n",
        "            north_of(r, r') :: \"The {r} is mapped north of {r'}\";\n",
        "            south_of(r, r') :: \"The {r} is mapped south of {r'}\";\n",
        "            east_of(r, r') :: \"The {r} is mapped east of {r'}\";\n",
        "            west_of(r, r') :: \"The {r} is mapped west of {r'}\";\n",
        "\n",
        "            north_of/d(r, d, r') :: \"South of {r} and north of {r'} is a door called {d}\";\n",
        "            south_of/d(r, d, r') :: \"North of {r} and south of {r'} is a door called {d}\";\n",
        "            east_of/d(r, d, r') :: \"West of {r} and east of {r'} is a door called {d}\";\n",
        "            west_of/d(r, d, r') :: \"East of {r} and west of {r'} is a door called {d}\";\n",
        "        }\n",
        "\n",
        "        commands {\n",
        "            go/north :: \"go north\" :: \"going north\";\n",
        "            go/south :: \"go south\" :: \"going south\";\n",
        "            go/east :: \"go east\" :: \"going east\";\n",
        "            go/west :: \"go west\" :: \"going west\";\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# key\n",
        "type k : o {\n",
        "    predicates {\n",
        "        match(k, c);\n",
        "        match(k, d);\n",
        "    }\n",
        "\n",
        "    constraints {\n",
        "        k1 :: match(k, c) & match(k', c) -> fail();\n",
        "        k2 :: match(k, c) & match(k, c') -> fail();\n",
        "        k3 :: match(k, d) & match(k', d) -> fail();\n",
        "        k4 :: match(k, d) & match(k, d') -> fail();\n",
        "    }\n",
        "\n",
        "    inform7 {\n",
        "        type {\n",
        "            kind :: \"key\";\n",
        "        }\n",
        "\n",
        "        predicates {\n",
        "            match(k, c) :: \"The matching key of the {c} is the {k}\";\n",
        "            match(k, d) :: \"The matching key of the {d} is the {k}\";\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# thing\n",
        "type t {\n",
        "    rules {\n",
        "        examine/t :: at(P, r) & $at(t, r) -> at(P, r);\n",
        "    }\n",
        "\n",
        "    reverse_rules {\n",
        "        examine/t :: examine/t;\n",
        "    }\n",
        "\n",
        "    inform7 {\n",
        "        type {\n",
        "            kind :: \"thing\";\n",
        "        }\n",
        "\n",
        "        commands {\n",
        "            examine/t :: \"examine {t}\" :: \"examining the {t}\";\n",
        "        }\n",
        "    }\n",
        "}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-N0-ntOwPEC-"
      },
      "outputs": [],
      "source": [
        "#@title twl 2\n",
        "twl += '''\n",
        "# food\n",
        "type f : o {\n",
        "    predicates {\n",
        "        edible(f);\n",
        "\t\tdrinkable(f);\n",
        "        eaten(f);\n",
        "    }\n",
        "\n",
        "    rules {\n",
        "        eat :: in(f, I) & edible(f) -> eaten(f);\n",
        "\t\tdrink :: in(f, I) & drinkable(f) -> eaten(f);\n",
        "    }\n",
        "\n",
        "    constraints {\n",
        "        eaten1 :: eaten(f) & in(f, I) -> fail();\n",
        "        eaten2 :: eaten(f) & in(f, c) -> fail();\n",
        "        eaten3 :: eaten(f) & on(f, s) -> fail();\n",
        "        eaten4 :: eaten(f) & at(f, r) -> fail();\n",
        "    }\n",
        "\n",
        "    inform7 {\n",
        "        type {\n",
        "            kind :: \"food\";\n",
        "            definition :: \"food can be edible. food can be drinkable. food can be eaten.\";\n",
        "        }\n",
        "\n",
        "        predicates {\n",
        "            edible(f) :: \"The {f} is edible\";\n",
        "\t\t\tdrinkable(f) :: \"The {f} is drinkable\";\n",
        "            eaten(f) :: \"The {f} is eaten\";\n",
        "        }\n",
        "\n",
        "        commands {\n",
        "            eat :: \"eat {f}\" :: \"eating the {f}\";\n",
        "\t\t\tdrink :: \"drink {f}\" :: \"drinking the {f}\";\n",
        "        }\n",
        "\n",
        "\t\tcode :: \"\"\"\n",
        "            [Drinking liquid]\n",
        "            The block drinking rule is not listed in any rulebook.\n",
        "            Report an actor drinking carried thing (this is the report drinking rule):\n",
        "                if the actor is the player:\n",
        "                    say \"You drink [the noun]. Not bad.\";\n",
        "                otherwise:\n",
        "                    say \"[The person asked] just drank [the noun].\".\n",
        "            After drinking the noun:\n",
        "                now the noun is eaten;\n",
        "            After eating the noun:\n",
        "                now the noun is eaten;\n",
        "        \"\"\";\n",
        "\n",
        "    }\n",
        "}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bL00PQEc3q8Q"
      },
      "outputs": [],
      "source": [
        "#@title Update grammar files with props from new logic\n",
        "write_grammar = False #@param {'type': 'boolean'}\n",
        "if write_grammar:\n",
        "    path = \"/../content/grammars/\"\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    for i in range(len(vars(grammar)[\"grammar_files\"])):\n",
        "        with open(vars(grammar)[\"grammar_files\"][i]) as f:\n",
        "            with open(f\"{path}{options.grammar.theme}{i}.twg\", \"w\") as f2:\n",
        "                lines = f.readlines()\n",
        "                for line in lines:\n",
        "                    if line.startswith(\"##actions\"):\n",
        "                        #f2.write(line)\n",
        "                        f2.write(\"wear:wear (o).\\n\")\n",
        "                        f2.write(\"steal:steal (o).\\n\")\n",
        "                        f2.write(\"wield:wield (o).\\n\")\n",
        "                        f2.write(\"drink:drink (o).\\n\")\n",
        "                        f2.write(\"eat:eat (o).\\n\")\n",
        "                        f2.write(\"hit:hit (o).\\n\")\n",
        "                        f2.write(\"hug:hug (o).\\n\")\n",
        "                        f2.write(\"use:use (o).\\n\")\n",
        "                        f2.write(\"follow:follow (o).\\n\")\n",
        "                    #else:\n",
        "                    #    f2.write(line)\n",
        "# %cp /../content/grammars/ /../content/drive/MyDrive/data/light_data/grammars/ -r\n",
        "\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHoJ2M1XiGW8"
      },
      "outputs": [],
      "source": [
        "logic = GameLogic.parse(twl)\n",
        "options = GameOptions()\n",
        "options.kb = KnowledgeBase(logic, f\"{DATA_PTH}grammars/\")\n",
        "rngs = options.rngs\n",
        "rng_quest = rngs['quest']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc_80zjwuiNz"
      },
      "source": [
        "### Create games"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Q2eXniaUHSLD"
      },
      "outputs": [],
      "source": [
        "#@title ### Make games current v3 affordance-based\n",
        "M = GameMaker(options)\n",
        "min_quest_len = 5 #@param {'type': 'integer'}\n",
        "max_quest_len = 5 #@param {'type': 'integer'}\n",
        "\n",
        "ix = 101 #@param {type:\"slider\", min:0, max:7500, step:1}\n",
        "maxix = 0 #@param {'type': 'integer'}\n",
        "\n",
        "g = \"F\" #@param [\"M\", \"F\"]\n",
        "rewards = \"balanced\" #@param [\"sparse\", \"balanced\", \"dense\"]\n",
        "gen_bulk = False #@param {'type': 'boolean'}\n",
        "restrict_len = False #@param {'type': 'boolean'}\n",
        "save = False #@param {'type': 'boolean'}\n",
        "global_based = False #@param {'type': 'boolean'}\n",
        "quest_based = True #@param {'type': 'boolean'}\n",
        "alt_distractors = True #@param {'type': 'boolean'}\n",
        "merged_dff = dfqq_quest_global_graph\n",
        "qt = \"w\" #@param [\"w\", \"f\"]\n",
        "merged_dff = merged_dff[dfqq_quest_global_graph.gender == g]\n",
        "merged_dff = merged_dff.reset_index(inplace=False)\n",
        "post_d = {\"wear\": \"worn\", \"wield\": \"wielded\",\n",
        "          \"eat\": \"eaten\", \"drink\": \"eaten\"}\n",
        "supporters = [\"huggable\", \"hittable\", \"followable\", \"goable\"]\n",
        "if qt == \"w\":\n",
        "    va, vb = \"wield\", \"wear\"\n",
        "elif qt == \"f\":\n",
        "    va, vb = \"eat\", \"drink\"\n",
        "pset = {'drink': \"drinkable\",'drop': \"o\",'eat': \"edible\",\n",
        "        'follow': \"followable\", 'get': \"o\", 'give': \"o\",\n",
        "        'go': \"goable\", 'hit': \"hittable\", 'hug': \"huggable\",\n",
        "        'put': \"o\", 'remove': \"o\", 'steal': \"o\",\n",
        "        'use': \"o\", 'wear': \"wearable\", 'wield': \"wieldable\"}\n",
        "container_d = {\"wield\": ['wooden chest', 'metal locker'],\n",
        "                'wear': ['large closet', 'antique wardrobe'],\n",
        "                'drink': ['icebox', 'cooler'],\n",
        "                'eat': ['large pantry', 'cabinet']}\n",
        "# Use pairs if wield/wear in quest, otherwise use global, else skip\n",
        "\n",
        "if maxix == 0:\n",
        "    maxix = len(merged_dff)\n",
        "rnge = range(maxix) if gen_bulk else [ix]\n",
        "if gen_bulk:\n",
        "    dct = {\"walkthrough\": [None for _ in rnge],\n",
        "        \"objective\": [None for _ in rnge],\n",
        "        \"quests\": [None for _ in rnge]}\n",
        "else:\n",
        "    dct = {\"walkthrough\": [None for _ in range(ix+1)],\n",
        "        \"objective\": [None for _ in range(ix+1)],\n",
        "        \"quests\": [None for _ in range(ix+1)]}\n",
        "if rnge == [ix] and ix >= len(merged_dff):\n",
        "    rnge = [len(merged_dff)-1]\n",
        "for ix in rnge:\n",
        "    pairs = merged_dff.iloc[ix].pairs\n",
        "    global_pairs = merged_dff.iloc[ix].pairs_global\n",
        "    if quest_based:\n",
        "        pairs_d = pairs\n",
        "    elif global_based:\n",
        "        pairs_d = global_pairs\n",
        "    va_avail = any([va in vs for _, vs in pairs_d.items()]) # any wield cmds\n",
        "    vb_avail = any([vb in vs for _, vs in pairs_d.items()]) # '' wear ''\n",
        "    if not va_avail and not vb_avail:\n",
        "        continue\n",
        "    walkthrough = []\n",
        "    quests = []\n",
        "\n",
        "    if ix >= len(merged_dff):\n",
        "        ix = len(merged_dff) - 1\n",
        "        print(f\"Max index of {len(merged_dff)-1} exceeded. Set index to {ix}.\")\n",
        "    else:\n",
        "        va_list = []\n",
        "        vb_list = []\n",
        "        rooms = []\n",
        "        room_descs = dict()\n",
        "        M = GameMaker(options)\n",
        "\n",
        "        if type(merged_dff.iloc[ix].graph_json) == float:\n",
        "            continue\n",
        "        rix = 0\n",
        "        for node, d in merged_dff.iloc[ix].graph_json[\"nodes\"].items():\n",
        "            if d[\"room\"]: # hacky replacements to prevent errors\n",
        "                room = M.new_room(name = d[\"name\"].replace(\"outside\", \"ouitside\")) #, desc=d[\"desc\"])\n",
        "                room_descs[f\"r_{rix}\"] = d[\"desc\"].replace(\"\\\"\", \"\\'\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "                rix += 1\n",
        "                rooms.append(room)\n",
        "        M.set_player(rooms[0])\n",
        "\n",
        "        chest = M.new(type='c', name=container_d[va][0])\n",
        "        locker = M.new(type='c', name=container_d[va][1])\n",
        "        rooms[0].add(chest, locker)\n",
        "        locker.add_property(\"closed\")\n",
        "        chest.add_property(\"closed\")\n",
        "\n",
        "        if rng_quest.rand() > 0.5:\n",
        "            lholder = chest\n",
        "        else:\n",
        "            lholder = locker\n",
        "\n",
        "        closet = M.new(type='c', name=container_d[vb][0])\n",
        "        wardrobe = M.new(type='c', name=container_d[vb][1])\n",
        "        rooms[0].add(closet, wardrobe)\n",
        "        wardrobe.add_property(\"closed\")\n",
        "        closet.add_property(\"closed\")\n",
        "\n",
        "        for container in M.findall(type=\"c\"):\n",
        "            container.add_property(\"closed\")\n",
        "        if rng_quest.rand() > 0.5:\n",
        "            holder = closet\n",
        "        else:\n",
        "            holder = wardrobe\n",
        "        tsd = dict()\n",
        "        for o, vs in pairs_d.items():\n",
        "            if va in vs or vb in vs:\n",
        "                if va in vs:\n",
        "                    if o in tsd.keys():\n",
        "                        tsd[o].add(pset[va])\n",
        "                    else:\n",
        "                        tsd[o] = {pset[va]}\n",
        "                if vb in vs:\n",
        "                    if o in tsd.keys():\n",
        "                        tsd[o].add(pset[vb])\n",
        "                    else:\n",
        "                        tsd[o] = {pset[vb]}\n",
        "        for o, ts in tsd.items():\n",
        "            old_o = o # may modify later but still want to look up w/ orig\n",
        "            if any([t in supporters for t in ts]): # fixed in place\n",
        "                t = \"s\" # will change to \"o\" if target affordance(s)\n",
        "                if ts == {'goable'}: # e.g. don't treat a castle as portable\n",
        "                    continue\n",
        "            else:\n",
        "                if \"edible\" in ts or \"drinkable\" in ts:\n",
        "                    t = \"f\"\n",
        "                else:\n",
        "                    t = \"o\"\n",
        "            properties = [] # reset to prevent properties carrying over to supporters\n",
        "            if not o[-1].isalpha():\n",
        "                o = o[:-1]\n",
        "\n",
        "            if pset[va] in ts and pset[vb] in ts: # choose 1 based on freq\n",
        "                # to avoid conflicts\n",
        "                if len(vod_df_qs[g][va][old_o]) > len(vod_df_qs[g][vb][old_o]):\n",
        "                    properties = [pset[va]]\n",
        "                elif len(vod_df_qs[g][va][old_o]) < len(vod_df_qs[g][vb][old_o]):\n",
        "                    properties = [pset[vb]]\n",
        "                else:\n",
        "                    if rng_quest.rand() > 0.5:\n",
        "                        properties = [pset[va]]\n",
        "                    else:\n",
        "                        properties = [pset[vb]]\n",
        "            elif pset[va] in ts: # e.g. only has wieldable/wearable\n",
        "                properties = [pset[va]]\n",
        "                if not va in [\"eat\", \"drink\"]:\n",
        "                    t = \"o\" # e.g., wine can be wielded or consumed, prioritize target, esp. since consuming destroys\n",
        "                else:\n",
        "                    t = \"f\"\n",
        "            elif pset[vb] in ts:\n",
        "                properties = [pset[vb]]\n",
        "                if not vb in [\"eat\", \"drink\"]:\n",
        "                    t = \"o\"\n",
        "                else:\n",
        "                    t = \"f\"\n",
        "            else: # free to add non-target w/o fear of container/quest conflicts\n",
        "                properties = [t for t in ts if len(t) > 1]\n",
        "            o2 = M.new(type=t, name=o)\n",
        "            if properties:\n",
        "                for p in properties:\n",
        "                    o2.add_property(p)\n",
        "            o2.infos.desc = o # default description is name - use affordances?\n",
        "\n",
        "            for _, d in merged_dff.iloc[ix].graph_json[\"nodes\"].items():\n",
        "                if o in d[\"name\"].lower(): # use original description if available\n",
        "                    o2.infos.desc = d[\"desc\"].replace(\"\\\"\", \"\\'\").strip()\n",
        "                    break\n",
        "            in_holder = False\n",
        "            in_lholder = False\n",
        "            if o2.has_property(pset[va]):\n",
        "                va_list.append(o2)\n",
        "                if not o2 in lholder.content:\n",
        "                    lholder.add(o2)\n",
        "                    in_lholder = True\n",
        "                    if alt_distractors:\n",
        "                        distractor = random.choice(list(vod_df_qs[g][vb].keys()))\n",
        "                        while distractor in vod_df_qs[g][va]:\n",
        "                            distractor = random.choice(list(vod_df_qs[g][vb].keys()))\n",
        "                        distractor = M.new(type=lholder.content[0].type, name=distractor)\n",
        "                        distractor.add_property(pset[va])\n",
        "                        lholder.add(distractor)\n",
        "            elif o2.has_property(pset[vb]):\n",
        "                vb_list.append(o2)\n",
        "                if not o2 in holder.content:\n",
        "                    holder.add(o2)\n",
        "                    in_holder = True\n",
        "                    if alt_distractors:\n",
        "                        distractor = random.choice(list(vod_df_qs[g][va].keys()))\n",
        "                        while distractor in vod_df_qs[g][vb]:\n",
        "                            distractor = random.choice(list(vod_df_qs[g][va].keys()))\n",
        "                        distractor = M.new(type=holder.content[0].type, name=distractor)\n",
        "                        distractor.add_property(pset[vb])\n",
        "                        holder.add(distractor)\n",
        "\n",
        "        # create walkthrough, quest\n",
        "        if len(holder.content) > 0:\n",
        "            if vb_list:\n",
        "                walkthrough.append(f\"open {holder.name}\")\n",
        "                if rewards == \"dense\":\n",
        "                    quests.append(\n",
        "                        Quest(win_events=[\n",
        "                            Event(conditions={M.new_fact(\"open\", holder)})\n",
        "                        ])\n",
        "                    )\n",
        "        if len(lholder.content) > 0:\n",
        "            if va_list:\n",
        "                walkthrough.append(f\"open {lholder.name}\")\n",
        "                if rewards == \"dense\":\n",
        "                    quests.append(\n",
        "                        Quest(win_events=[\n",
        "                            Event(conditions={M.new_fact(\"open\", lholder)})\n",
        "                        ])\n",
        "                    )\n",
        "        # balanced, dense only reward higher-level, not conditioning affordances\n",
        "        if vb_list:\n",
        "            for w in vb_list:\n",
        "                if w in holder.content:\n",
        "                    walkthrough.append(f\"take {w.name} from {holder.name}\")\n",
        "                    if rewards in [\"balanced\", \"dense\"]:\n",
        "                        quests.append(\n",
        "                            Quest(win_events=[\n",
        "                                Event(conditions={M.new_fact(\"in\", w,\n",
        "                                                            M.inventory)})\n",
        "                            ])\n",
        "                        )\n",
        "\n",
        "                    walkthrough.append(f\"{vb} {w.name}\")\n",
        "                    quests.append(\n",
        "                        Quest(win_events=[\n",
        "                            Event(conditions={M.new_fact(f\"{post_d[vb]}\", w)})\n",
        "                        ])\n",
        "                    )\n",
        "        if va_list:\n",
        "            for w in va_list:\n",
        "                if w in lholder.content:\n",
        "                    walkthrough.append(f\"take {w.name} from {lholder.name}\")\n",
        "                    if rewards in [\"balanced\", \"dense\"]:\n",
        "                        quests.append(\n",
        "                            Quest(win_events=[\n",
        "                                Event(conditions={M.new_fact(\"in\", w, M.inventory)})\n",
        "                            ])\n",
        "                        )\n",
        "\n",
        "                    walkthrough.append(f\"{va} {w.name}\")\n",
        "                    quests.append(\n",
        "                        Quest(win_events=[\n",
        "                            Event(conditions={M.new_fact(f\"{post_d[va]}\", w)})\n",
        "                        ])\n",
        "                    )\n",
        "\n",
        "        if restrict_len: # e.g. only quests of length 5 in min=max=5\n",
        "            cond = len(quests) in range(min_quest_len, max_quest_len+1)\n",
        "        else:\n",
        "            cond = len(quests) > 0\n",
        "        if cond: # no quests available\n",
        "            M.quests = quests\n",
        "            if gen_bulk and ix > 1 and ix % 50 == 0: # log\n",
        "                print(ix, walkthrough)\n",
        "            if not gen_bulk:\n",
        "                print(ix, g, walkthrough)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        if not rewards in [\"balanced\", \"dense\"]: # buggy on 'take'\n",
        "            M.set_walkthrough(walkthrough)\n",
        "\n",
        "        game = M.build()\n",
        "        for rid, desc in room_descs.items():\n",
        "            game.infos[rid].desc = desc + \"\\n\" + game.infos[rid].desc\n",
        "        game.metadata[\"walkthrough\"] = walkthrough\n",
        "        game.objective = \"\"\n",
        "        if va_list:\n",
        "            for w in va_list:\n",
        "                game.objective += f\"Find and {va} {w.name}. \"\n",
        "        if vb_list:\n",
        "            for w in vb_list:\n",
        "                game.objective += f\"Find and {vb} {w.name}. \"\n",
        "        if save:\n",
        "            if global_based:\n",
        "                ssuffix = \"_gb\"\n",
        "            elif quest_based:\n",
        "                ssuffix = \"_qb\"\n",
        "            ssuffix += \"_nod\"\n",
        "            pth = f\"/../content/{g.lower()}_quests_{qt}_{rewards}{ssuffix}/\"\n",
        "            if not os.path.exists(pth):\n",
        "                os.makedirs(pth)\n",
        "            try:\n",
        "                M.compile(f\"{pth}/test_{ix}\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        dct[\"walkthrough\"][ix] = walkthrough\n",
        "        dct[\"quests\"][ix] = quests\n",
        "        dct[\"objective\"][ix] = game.objective\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q53535WJuVb4"
      },
      "outputs": [],
      "source": [
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8isjdr8nWqY"
      },
      "outputs": [],
      "source": [
        "# !zip -r /../content/m_quests_f_balanced_qb_nod.zip /../content/m_quests_f_balanced_qb_nod/ &> /dev/null\n",
        "# !zip -r /../content/f_quests_f_balanced_qb_nod.zip /../content/f_quests_f_balanced_qb_nod/ &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxAeCRAloiXn"
      },
      "outputs": [],
      "source": [
        "# %cp /../content/m_quests_f_balanced_qb_nod.zip {{DATA_PTH}}\n",
        "# %cp /../content/f_quests_f_balanced_qb_nod.zip {{DATA_PTH}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTfguve1hQZ6"
      },
      "outputs": [],
      "source": [
        "M.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5EixY1t62Qc2"
      },
      "outputs": [],
      "source": [
        "#@title Load/save walkthroughs\n",
        "\n",
        "gender = \"f\" #@param [\"m\", \"f\"]\n",
        "basis = \"_qb\" #@param [\"\", \"_qb\", \"_gb\"]\n",
        "distractors = \"_nod\" #@param [\"\", \"_nod\", \"_d\", \"_df\"]\n",
        "mode = \"w\" #@param [\"w\", \"f\"]\n",
        "\n",
        "save_walkthroughs = False #@param {'type': 'boolean'}\n",
        "load_walkthroughs = True #@param {'type': 'boolean'}\n",
        "\n",
        "if save_walkthroughs:\n",
        "    pd.DataFrame(dct).to_pickle(f\"{DATA_PTH}quests/{gender}_quests_{mode}{basis}{distractors}_walkthroughs.pkl\")\n",
        "\n",
        "if load_walkthroughs:\n",
        "    m_walkthroughs = pd.read_pickle(f\"{DATA_PTH}quests/m_quests_{mode}{basis}{distractors}_walkthroughs.pkl\")\n",
        "    m_walkthroughs = m_walkthroughs.dropna(0)\n",
        "    f_walkthroughs = pd.read_pickle(f\"{DATA_PTH}quests/f_quests_{mode}{basis}{distractors}_walkthroughs.pkl\")\n",
        "    f_walkthroughs = f_walkthroughs.dropna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzMsFkW8Y4Uv"
      },
      "source": [
        "### Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yUIlwMbrRapV"
      },
      "outputs": [],
      "source": [
        "#@title Definitions\n",
        "\n",
        "seed = 42 #@param {'type': 'integer'}\n",
        "\n",
        "def statistic(x, y, axis):\n",
        "    \"\"\"Test statistic for permutation tests.\"\"\"\n",
        "    return np.mean(x, axis=axis) - np.mean(y, axis=axis)\n",
        "\n",
        "def get_res(a, b, alpha=0.01, alt='greater', seed=42, num_its=1000):\n",
        "    res = scipy.stats.permutation_test(data=[a, b],\n",
        "                                            statistic=statistic,\n",
        "                                            n_resamples=num_its,\n",
        "                                            permutation_type='samples',\n",
        "                                            alternative=alt,\n",
        "                                            random_state=seed)\n",
        "    mean_diff = res.statistic\n",
        "    pval = res.pvalue\n",
        "    d = pg.compute_effsize(a, b, eftype='cohen')\n",
        "    if alt == 'greater':\n",
        "        cles = pg.compute_effsize(a, b, eftype='cles')\n",
        "    elif alt == 'less':\n",
        "        cles = pg.compute_effsize(b, a, eftype='cles')\n",
        "    else:\n",
        "        cles = 0\n",
        "\n",
        "    return {\"diff\": mean_diff, \"pval\": pval, \"sig\": pval < alpha, \"d\": d, \"cles\": cles, 'alt': alt}\n",
        "\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "def get_split_files(pth, split_indices):\n",
        "    return [f\"{pth}test_{ix}.ulx\"\n",
        "            for ix in split_indices\n",
        "            if os.path.isfile(f\"{pth}test_{ix}.ulx\")]\n",
        "\n",
        "def get_split_indices(pth, g, split):\n",
        "    with open(f\"{pth}{g}_{split}_{qtype}_{rewards}{basis}{distractors}.txt\", \"r\") as f:\n",
        "        all_g = f.read().splitlines()\n",
        "    return all_g\n",
        "\n",
        "def get_combined(m_ulx, f_ulx):\n",
        "    indicesA = list(set(map(lambda g: re.search(r\"test_[0-9]+\\.\", g)[0].replace(\".\", \"_m\"), m_ulx)))\n",
        "    indicesB = list(set(map(lambda g: re.search(r\"test_[0-9]+\\.\", g)[0].replace(\".\", \"_f\"), f_ulx)))\n",
        "    if len(indicesA) >= len(indicesB):\n",
        "        alli = indicesA[:len(indicesB)] + indicesB\n",
        "    else:\n",
        "        alli = indicesA + indicesB[:len(indicesA)]\n",
        "    all = []\n",
        "    for i in alli:\n",
        "        if \"_m\" in i:\n",
        "            for g in m_ulx:\n",
        "                if re.search(r\"test_[0-9]+\\.\", g)[0].replace(\".\", \"_m\") == i:\n",
        "                    all.append(g)\n",
        "        elif \"_f\" in i:\n",
        "            for g in f_ulx:\n",
        "                if re.search(r\"test_[0-9]+\\.\", g)[0].replace(\".\", \"_f\") == i:\n",
        "                    all.append(g)\n",
        "    return all\n",
        "\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "# modified via https://github.com/microsoft/TextWorld/blob/main/notebooks/Building%20a%20simple%20agent.ipynb\n",
        "def play(agent, path, gamefiles = None, max_step=100, nb_episodes=10, verbose=True, seed=42):\n",
        "    # For reproducibility\n",
        "    set_seed(seed)\n",
        "    infos_to_request = agent.infos_to_request\n",
        "    infos_to_request.max_score = True  # Needed to normalize the scores.\n",
        "    if not gamefiles and not type(path) == list:\n",
        "        gamefiles = [path]\n",
        "    elif not gamefiles:\n",
        "        gamefiles = path\n",
        "    if not type(gamefiles) == list or gamefiles[0] == path:\n",
        "        if os.path.isdir(path): # *.z8\n",
        "            gamefiles = glob(os.path.join(path, \"*.ulx\")) # should find .json on its own for metadata\n",
        "    else:\n",
        "        gamefiles = [g for g in gamefiles if g.endswith(\".ulx\")]\n",
        "        print(f\"Using provided list of gamefiles e.g., {gamefiles[0]}.\")\n",
        "    env_id = textworld.gym.register_games(gamefiles,\n",
        "                                          request_infos=infos_to_request,\n",
        "                                          max_episode_steps=max_step)\n",
        "    env = gym.make(env_id)  # Create a Gym environment to play the text game.\n",
        "    if verbose:\n",
        "        if os.path.isdir(path):\n",
        "            print(os.path.dirname(path), end=\"\")\n",
        "        else:\n",
        "            print(os.path.basename(path), end=\"\")\n",
        "\n",
        "    # Collect some statistics: nb_steps, final reward.\n",
        "    avg_moves, avg_scores, avg_norm_scores = [], [], []\n",
        "    trajs = []\n",
        "    for no_episode in range(nb_episodes):\n",
        "        obs, infos = env.reset()  # Start new episode.\n",
        "        score = 0\n",
        "        done = False\n",
        "        nb_moves = 0\n",
        "        traj = []\n",
        "        while not done:\n",
        "            command = agent.act(obs, score, done, infos)\n",
        "            traj.append(command)\n",
        "            obs, score, done, infos = env.step(command)\n",
        "            nb_moves += 1\n",
        "\n",
        "        agent.act(obs, score, done, infos)  # Let the agent know the game is done.\n",
        "\n",
        "        if verbose:\n",
        "            print(\".\", end=\"\")\n",
        "        avg_moves.append(nb_moves)\n",
        "        avg_scores.append(score)\n",
        "        avg_norm_scores.append(score / infos[\"max_score\"])\n",
        "        trajs.append(traj)\n",
        "    env.close()\n",
        "    if verbose:\n",
        "        if os.path.isdir(path):\n",
        "            msg = \"  \\tavg. steps: {:5.1f}; avg. normalized score: {:4.1f} / {}.\"\n",
        "            print(msg.format(np.mean(avg_moves), np.mean(avg_norm_scores), 1))\n",
        "        else:\n",
        "            msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n",
        "            print(msg.format(np.mean(avg_moves), np.mean(avg_scores), infos[\"max_score\"]))\n",
        "    return avg_moves, avg_norm_scores, trajs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy8swXYUeMwd"
      },
      "source": [
        "#### Agent (Neural)\n",
        "- [via](https://github.com/microsoft/TextWorld/blob/main/notebooks/Building%20a%20simple%20agent.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6UWgSsfReOxg"
      },
      "outputs": [],
      "source": [
        "#@title #### Classes, Imports\n",
        "# modified via: https://github.com/microsoft/TextWorld/blob/main/notebooks/Building%20a%20simple%20agent.ipynb\n",
        "import re\n",
        "from typing import List, Mapping, Any, Optional\n",
        "from collections import defaultdict\n",
        "warnings.filterwarnings(category=UserWarning,\n",
        "                                            action='ignore')\n",
        "warnings.filterwarnings(category=DeprecationWarning,\n",
        "                                            action='ignore')\n",
        "import numpy as np\n",
        "\n",
        "import textworld\n",
        "import textworld.gym\n",
        "from textworld import EnvInfos\n",
        "import torchtext\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class CommandScorer(nn.Module):\n",
        "    def __init__(self, train_seed, input_size, hidden_size):\n",
        "        super(CommandScorer, self).__init__()\n",
        "        set_seed(train_seed)  # For reproducibility\n",
        "        self.embedding    = nn.Embedding(input_size, hidden_size, device=device)\n",
        "        self.encoder_gru  = nn.GRU(hidden_size, hidden_size, device=device)\n",
        "        self.cmd_encoder_gru  = nn.GRU(hidden_size, hidden_size, device=device)\n",
        "        self.state_gru    = nn.GRU(hidden_size, hidden_size, device=device)\n",
        "        self.hidden_size  = hidden_size\n",
        "        self.state_hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
        "        self.critic       = nn.Linear(hidden_size, 1, device=device)\n",
        "        self.att_cmd      = nn.Linear(hidden_size * 2, 1, device=device)\n",
        "\n",
        "    def forward(self, obs, commands, **kwargs):\n",
        "        input_length = obs.size(0)\n",
        "        batch_size = obs.size(1)\n",
        "        nb_cmds = commands.size(1)\n",
        "\n",
        "        embedded = self.embedding(obs)\n",
        "        encoder_output, encoder_hidden = self.encoder_gru(embedded)\n",
        "        state_output, state_hidden = self.state_gru(encoder_hidden, self.state_hidden)\n",
        "        self.state_hidden = state_hidden\n",
        "        value = self.critic(state_output)\n",
        "\n",
        "        # Attention network over the commands.\n",
        "        cmds_embedding = self.embedding.forward(commands)\n",
        "        _, cmds_encoding_last_states = self.cmd_encoder_gru.forward(cmds_embedding)  # 1 x cmds x hidden\n",
        "\n",
        "        # Same observed state for all commands.\n",
        "        cmd_selector_input = torch.stack([state_hidden] * nb_cmds, 2)  # 1 x batch x cmds x hidden\n",
        "\n",
        "        # Same command choices for the whole batch.\n",
        "        cmds_encoding_last_states = torch.stack([cmds_encoding_last_states] * batch_size, 1)  # 1 x batch x cmds x hidden\n",
        "\n",
        "        # Concatenate the observed state and command encodings.\n",
        "        cmd_selector_input = torch.cat([cmd_selector_input, cmds_encoding_last_states], dim=-1)\n",
        "\n",
        "        # Compute one score per command.\n",
        "        scores = F.relu(self.att_cmd(cmd_selector_input)).squeeze(-1)  # 1 x Batch x cmds\n",
        "\n",
        "        probs = F.softmax(scores, dim=2)  # 1 x Batch x cmds\n",
        "        index = probs[0].multinomial(num_samples=1).unsqueeze(0) # 1 x batch x indx\n",
        "        return scores, index, value\n",
        "\n",
        "    def reset_hidden(self, batch_size):\n",
        "        self.state_hidden = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
        "\n",
        "class NeuralAgent:\n",
        "    \"\"\" Simple Neural Agent for playing TextWorld games. \"\"\"\n",
        "    MAX_VOCAB_SIZE = 1000\n",
        "    UPDATE_FREQUENCY = 10\n",
        "    LOG_FREQUENCY = 1000\n",
        "    GAMMA = 0.9\n",
        "\n",
        "    def __init__(self, train_seed=42) -> None:\n",
        "        self._initialized = False\n",
        "        self._epsiode_has_started = False\n",
        "        self.id2word = [\"<PAD>\", \"<UNK>\"]\n",
        "        self.word2id = {w: i for i, w in enumerate(self.id2word)}\n",
        "\n",
        "        self.model = CommandScorer(train_seed=train_seed, input_size=self.MAX_VOCAB_SIZE, hidden_size=128)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), 0.00003)\n",
        "\n",
        "        self.mode = \"train\"\n",
        "        self.results = []\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train(True)\n",
        "        self.mode = \"train\"\n",
        "        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
        "        self.transitions = []\n",
        "        self.model.reset_hidden(1)\n",
        "        self.last_score = 0\n",
        "        self.no_train_step = 0\n",
        "\n",
        "    def test(self):\n",
        "        self.mode = \"test\"\n",
        "        self.model.reset_hidden(1)\n",
        "        self.model.eval()\n",
        "\n",
        "    @property\n",
        "    def infos_to_request(self) -> EnvInfos:\n",
        "        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n",
        "                        won=True, lost=True)\n",
        "\n",
        "    def _get_word_id(self, word):\n",
        "        if word not in self.word2id:\n",
        "            if len(self.word2id) >= self.MAX_VOCAB_SIZE:\n",
        "                return self.word2id[\"<UNK>\"]\n",
        "\n",
        "            self.id2word.append(word)\n",
        "            self.word2id[word] = len(self.word2id)\n",
        "\n",
        "        return self.word2id[word]\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        # Simple tokenizer: strip out all non-alphabetic characters.\n",
        "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
        "        word_ids = list(map(self._get_word_id, text.split()))\n",
        "        return word_ids\n",
        "\n",
        "    def _process(self, texts):\n",
        "        texts = list(map(self._tokenize, texts))\n",
        "        max_len = max(len(l) for l in texts)\n",
        "        padded = np.ones((len(texts), max_len)) * self.word2id[\"<PAD>\"]\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "            padded[i, :len(text)] = text\n",
        "\n",
        "        padded_tensor = torch.from_numpy(padded).type(torch.long).to(device)\n",
        "        padded_tensor = padded_tensor.permute(1, 0) # Batch x Seq => Seq x Batch\n",
        "        return padded_tensor\n",
        "\n",
        "    def _discount_rewards(self, last_values):\n",
        "        returns, advantages = [], []\n",
        "        R = last_values.data\n",
        "        for t in reversed(range(len(self.transitions))):\n",
        "            rewards, _, _, values = self.transitions[t]\n",
        "            R = rewards + self.GAMMA * R\n",
        "            adv = R - values\n",
        "            returns.append(R)\n",
        "            advantages.append(adv)\n",
        "\n",
        "        return returns[::-1], advantages[::-1]\n",
        "\n",
        "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> Optional[str]:\n",
        "\n",
        "        # Build agent's observation: feedback + look + inventory.\n",
        "        input_ = \"{}\\n{}\\n{}\".format(obs, infos[\"description\"], infos[\"inventory\"])\n",
        "\n",
        "        # Tokenize and pad the input and the commands to chose from.\n",
        "        input_tensor = self._process([input_])\n",
        "        commands_tensor = self._process(infos[\"admissible_commands\"])\n",
        "\n",
        "        # Get our next action and value prediction.\n",
        "        outputs, indexes, values = self.model(input_tensor, commands_tensor)\n",
        "        action = infos[\"admissible_commands\"][indexes[0]]\n",
        "\n",
        "        if self.mode == \"test\":\n",
        "            if done:\n",
        "                self.model.reset_hidden(1)\n",
        "            return action\n",
        "\n",
        "        self.no_train_step += 1\n",
        "\n",
        "        if self.transitions:\n",
        "            reward = score - self.last_score  # Reward is the gain/loss in score.\n",
        "            self.last_score = score\n",
        "            if infos[\"won\"]:\n",
        "                reward += 100\n",
        "            if infos[\"lost\"]:\n",
        "                reward -= 100\n",
        "\n",
        "            self.transitions[-1][0] = reward  # Update reward information.\n",
        "\n",
        "        self.stats[\"max\"][\"score\"].append(score)\n",
        "        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
        "            # Update model\n",
        "            returns, advantages = self._discount_rewards(values)\n",
        "\n",
        "            loss = 0\n",
        "            for transition, ret, advantage in zip(self.transitions, returns, advantages):\n",
        "                reward, indexes_, outputs_, values_ = transition\n",
        "\n",
        "                advantage        = advantage.detach() # Block gradients flow here.\n",
        "                probs            = F.softmax(outputs_, dim=2)\n",
        "                log_probs        = torch.log(probs)\n",
        "                log_action_probs = log_probs.gather(2, indexes_)\n",
        "                policy_loss      = (-log_action_probs * advantage).sum()\n",
        "                value_loss       = (.5 * (values_ - ret) ** 2.).sum()\n",
        "                entropy     = (-probs * log_probs).sum()\n",
        "                loss += policy_loss + 0.5 * value_loss - 0.1 * entropy\n",
        "\n",
        "                self.stats[\"mean\"][\"reward\"].append(reward)\n",
        "                self.stats[\"mean\"][\"policy\"].append(policy_loss.item())\n",
        "                self.stats[\"mean\"][\"value\"].append(value_loss.item())\n",
        "                self.stats[\"mean\"][\"entropy\"].append(entropy.item())\n",
        "                self.stats[\"mean\"][\"confidence\"].append(torch.exp(log_action_probs).item())\n",
        "\n",
        "            if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
        "                msg = \"{:6d}. \".format(self.no_train_step)\n",
        "                msg += \"  \".join(\"{}: {: 3.3f}\".format(k, np.mean(v)) for k, v in self.stats[\"mean\"].items())\n",
        "                msg += \"  \" + \"  \".join(\"{}: {:2d}\".format(k, np.max(v)) for k, v in self.stats[\"max\"].items())\n",
        "                msg += \"  vocab: {:3d}\".format(len(self.id2word))\n",
        "                print(msg)\n",
        "                self.results.append(self.stats)\n",
        "                self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
        "\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), 40)\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            self.transitions = []\n",
        "            self.model.reset_hidden(1)\n",
        "        else:\n",
        "            # Keep information about transitions for Truncated Backpropagation Through Time.\n",
        "            self.transitions.append([None, indexes, outputs, values])  # Reward will be set on the next call\n",
        "\n",
        "        if done:\n",
        "            self.last_score = 0  # Will be starting a new episode. Reset the last score.\n",
        "\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Whaa_cewJx-o"
      },
      "source": [
        "### Create splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qIHX7C6Lsukj"
      },
      "outputs": [],
      "source": [
        "#@title Important folder paths before split, train, eval\n",
        "\n",
        "basis = \"_qb\" #@param [\"\", \"_qb\", \"_gb\"]\n",
        "qtype = \"w\" #@param [\"w\", \"f\"]\n",
        "rewards = \"balanced\" #@param [\"sparse\", \"balanced\", \"dense\"]\n",
        "distractors = \"_nod\" #@param [\"\", \"_nod\", \"_d\", \"_df\"]\n",
        "local_folder_pths = True #@param {'type': 'boolean'}\n",
        "if local_folder_pths:\n",
        "    fprefix = \"/../content/\"\n",
        "else:\n",
        "    fprefix = f\"{DATA_PTH}quests/\"\n",
        "\n",
        "\n",
        "m_train_folder = f\"{fprefix}m_train_{qtype}_{rewards}{basis}{distractors}/\"\n",
        "m_dev_folder = f\"{fprefix}m_dev_{qtype}_{rewards}{basis}{distractors}/\"\n",
        "m_test_folder = f\"{fprefix}m_test_{qtype}_{rewards}{basis}{distractors}/\"\n",
        "f_train_folder = f\"{fprefix}f_train_{qtype}_{rewards}{basis}{distractors}/\"\n",
        "f_dev_folder = f\"{fprefix}f_dev_{qtype}_{rewards}{basis}{distractors}/\"\n",
        "f_test_folder = f\"{fprefix}f_test_{qtype}_{rewards}{basis}{distractors}/\"\n",
        "all_dev_folder = f\"{fprefix}all_dev{basis}{distractors}/\"\n",
        "all_test_folder = f\"{fprefix}all_test{basis}{distractors}/\"\n",
        "m_checkpoints_folder = f\"{DATA_PTH}m_checkpoints/\"\n",
        "f_checkpoints_folder = f\"{DATA_PTH}f_checkpoints/\"\n",
        "m_local_pth = f\"/../content/m_quests_{qtype}_{rewards}{basis}{distractors}/\"\n",
        "f_local_pth = f\"/../content/f_quests_{qtype}_{rewards}{basis}{distractors}/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oufdv1BUp9hs"
      },
      "outputs": [],
      "source": [
        "%cp {{DATA_PTH}}m_quests_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.zip /../content/\n",
        "%cp {{DATA_PTH}}f_quests_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.zip /../content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVIJm13Yp9hr"
      },
      "outputs": [],
      "source": [
        "m_quest_zip = f\"m_quests_{qtype}_{rewards}{basis}{distractors}.zip\"\n",
        "f_quest_zip = f\"f_quests_{qtype}_{rewards}{basis}{distractors}.zip\"\n",
        "!unzip \"$m_quest_zip\" -d /../ &> /dev/null\n",
        "!unzip \"$f_quest_zip\" -d /../ &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CpLbp1XkrNji"
      },
      "outputs": [],
      "source": [
        "#@title Create splits for each gender\n",
        "\n",
        "for g in [\"m\", \"f\"]:\n",
        "    g_local_pth = m_local_pth if g == \"m\" else f_local_pth\n",
        "\n",
        "    games = glob(os.path.join(g_local_pth, \"*.json\")) + \\\n",
        "            glob(os.path.join(g_local_pth, \"*.ulx\")) + \\\n",
        "            glob(os.path.join(g_local_pth, \"*.ni\"))\n",
        "    game_idxs = list(set(list(map(lambda g: re.search(r\"test_[0-9]+\",\n",
        "                                            g)[0].split(\"_\")[-1],\n",
        "                            games))))\n",
        "    train, test = train_test_split(game_idxs, shuffle = True,\n",
        "                                   random_state = 42, test_size=0.3)\n",
        "    dev, test = train_test_split(test, shuffle = True,\n",
        "                                 random_state = 42, test_size=0.50)\n",
        "    if g == \"m\":\n",
        "        m_train, m_dev, m_test = train, dev, test\n",
        "    elif g == \"f\":\n",
        "        f_train, f_dev, f_test = train, dev, test\n",
        "\n",
        "\n",
        "display(len(f_train), len(f_dev), len(f_test),\n",
        "        len(m_train), len(m_dev), len(m_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Qedby__UcZjG"
      },
      "outputs": [],
      "source": [
        "#@title Write gamefiles to text file and save to drive\n",
        "save_to_drive = False #@param {'type': 'boolean'}\n",
        "\n",
        "for g in [\"m\", \"f\"]:\n",
        "    g_train = m_train if g == \"m\" else f_train\n",
        "    g_dev = m_dev if g == \"m\" else f_dev\n",
        "    g_test = m_test if g == \"m\" else f_test\n",
        "\n",
        "    with open(f\"{g}_train_{qtype}_{rewards}{basis}{distractors}.txt\", \"w\") as f:\n",
        "        for i in g_train:\n",
        "            f.write(f\"{i}\\n\")\n",
        "    with open(f\"{g}_dev_{qtype}_{rewards}{basis}{distractors}.txt\", \"w\") as f:\n",
        "        for i in g_dev:\n",
        "            f.write(f\"{i}\\n\")\n",
        "    with open(f\"{g}_test_{qtype}_{rewards}{basis}{distractors}.txt\", \"w\") as f:\n",
        "        for i in g_test:\n",
        "            f.write(f\"{i}\\n\")\n",
        "if save_to_drive:\n",
        "    %cp m_train_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.txt {{DATA_PTH}}\n",
        "    %cp m_dev_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.txt {{DATA_PTH}}\n",
        "    %cp m_test_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.txt {{DATA_PTH}}\n",
        "    %cp f_train_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.txt {{DATA_PTH}}\n",
        "    %cp f_dev_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.txt {{DATA_PTH}}\n",
        "    %cp f_test_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.txt {{DATA_PTH}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuGGgFTxKAcF"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzxox_UlMKlr"
      },
      "outputs": [],
      "source": [
        "# train_seeds = [random.randint(1, 10000) for _ in range(10)]\n",
        "train_seeds = [858, 4386, 1429, 6673, 4368, 7131, 1719, 834, 2968, 7897]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy train indices lists to this runtime to consult when obtaining file lists (which will reference unzipped files in local folders after copying zips from drive to here)."
      ],
      "metadata": {
        "id": "fwcvii3fKumv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHpjN0qNX9mu"
      },
      "outputs": [],
      "source": [
        "m_train_txt = f\"{DATA_PTH}m_train_{qtype}_{rewards}{basis}{distractors}.txt\"\n",
        "f_train_txt = f\"{DATA_PTH}f_train_{qtype}_{rewards}{basis}{distractors}.txt\"\n",
        "all_train_txt = f\"{DATA_PTH}all_train_{qtype}_{rewards}{basis}{distractors}.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvvOvGj2yh02"
      },
      "outputs": [],
      "source": [
        "%cp {{m_train_txt}} /../content\n",
        "%cp {{f_train_txt}} /../content\n",
        "%cp {{all_train_txt}} /../content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6CYpHPUqMe69"
      },
      "outputs": [],
      "source": [
        "#@title Train male, female neural agents w/ multiple initial seeds\n",
        "ckpt = 100 # param {'type': 'integer'}\n",
        "k = 5 #@param {'type': 'integer'} # multiplier of training_games[:n] for nb. episodes\n",
        "continue_from_chkpt = False #@param {'type': 'boolean'}\n",
        "save = False #@param {'type': 'boolean'}\n",
        "save_res = False #@param {'type': 'boolean'}\n",
        "limit_n = 50 #@param {'type': 'integer'}\n",
        "use_limit = False #@param {'type': 'boolean'}\n",
        "use_all = True #@param {'type': 'boolean'}\n",
        "create_train_ulxs = True #@param {'type': 'boolean'}\n",
        "prefix = \"/../content/\"\n",
        "\n",
        "if create_train_ulxs:\n",
        "    try:\n",
        "        m_train_ulx = get_split_files(m_local_pth,\n",
        "                                    get_split_indices(\"/../content/\",\n",
        "                                                        \"m\", \"train\"))\n",
        "        f_train_ulx = get_split_files(f_local_pth,\n",
        "                                    get_split_indices(\"/../content/\",\n",
        "                                                        \"f\", \"train\"))\n",
        "    except FileNotFoundError as e:\n",
        "        split_pref = f\"{DATA_PTH}\"\n",
        "        m_train_ulx = get_split_files(m_local_pth,\n",
        "                                    get_split_indices(split_pref,\n",
        "                                                        \"m\", \"train\"))\n",
        "        f_train_ulx = get_split_files(f_local_pth,\n",
        "                                    get_split_indices(split_pref,\n",
        "                                                        \"f\", \"train\"))\n",
        "else:\n",
        "    try:\n",
        "        f_train_ulx[0]\n",
        "    except:\n",
        "        with open(f\"{DATA_PTH}m_train_{qtype}_{rewards}{basis}{distractors}.txt\", \"r\") as f:\n",
        "            m_train_ulx = f.read().splitlines()\n",
        "        with open(f\"{DATA_PTH}train_res/f_train_qb_nod_ulx.txt\", \"r\") as f:\n",
        "            # f\"{DATA_PTH}f_train_{qtype}_{rewards}{basis}{distractors}.txt\", \"r\")\n",
        "            f_train_ulx = f.read().splitlines()\n",
        "\n",
        "if continue_from_chkpt:\n",
        "    assert(n - ckpt > 0)\n",
        "\n",
        "train_mode = \"w_balanced_qb_nod\" #@param [\"w_balanced\", \"wear_balanced_qb_nod\", \"wield_balanced_qb_nod\", \"w_balanced_qb_nod\", \"w_balanced_qb_d\", \"w_balanced_gb_nod\", \"w_balanced_gb_d\", \"w_balanced_gb_df\", \"f_balanced_qb_nod\"]\n",
        "if \"wear\" in train_mode:\n",
        "    m_train_files = get_split_files(m_local_pth, m_train_wear_ixs)\n",
        "    f_train_files = get_split_files(f_local_pth, f_train_wear_ixs)\n",
        "elif \"wield\" in train_mode:\n",
        "    m_train_files = get_split_files(m_local_pth, m_train_wield_ixs)\n",
        "    f_train_files = get_split_files(f_local_pth, f_train_wield_ixs)\n",
        "else:\n",
        "    m_train_files = m_train_ulx\n",
        "    f_train_files = f_train_ulx\n",
        "\n",
        "if use_limit:\n",
        "    n = min(limit_n, min(len(m_train_files), len(f_train_files)))\n",
        "else:\n",
        "    n = min(len(m_train_files), len(f_train_files))\n",
        "\n",
        "single_seed  = False #@param {'type': 'boolean'}\n",
        "seed_from_list = \"858\" #@param train_seeds = [858, 4386, 1429, 6673, 4368, 7131, 1719, 834, 2968, 7897]\n",
        "seed_from_list = int(seed_from_list)\n",
        "seed_from_list = [seed_from_list]\n",
        "\n",
        "g_results = []\n",
        "\n",
        "for ggg in [\"M\", \"F\"]:\n",
        "    results = []\n",
        "    if continue_from_chkpt:\n",
        "        pth = f\"{prefix}{ggg.lower()}_training_games_{ckpt}_{n}{basis}{distractors}/\"\n",
        "    else:\n",
        "        pth = f\"{prefix}{ggg.lower()}_training_games_{train_mode}_{n}{basis}{distractors}/\"\n",
        "    if not use_all:\n",
        "        if ggg == \"M\":\n",
        "            training_games = m_train_files\n",
        "        else:\n",
        "            training_games = f_train_files\n",
        "        training_games = training_games[:n]\n",
        "    else:\n",
        "        ggg = \"All\"\n",
        "        # For parity with m/f, will want to cap at largest (m or f)\n",
        "        # For all, use half of each to reach cap\n",
        "        # e.g. cap at 240 b/c one gender only has 240,\n",
        "        # use 120 m, 120 f\n",
        "        j = n//2\n",
        "        training_games = m_train_ulx[:j] + f_train_ulx[:j]\n",
        "    seed_range = train_seeds if not single_seed else seed_from_list\n",
        "    overall_res = {\"m\": {ts: [] for ts in seed_range},\n",
        "                \"f\": {ts: [] for ts in seed_range}}\n",
        "    for train_seed in seed_range: # train_seeds:\n",
        "        overall_res[ggg.lower()][train_seed] = []\n",
        "        train_seed = int(train_seed)\n",
        "        set_seed(train_seed)\n",
        "        agent = NeuralAgent(train_seed=train_seed)\n",
        "        if continue_from_chkpt:\n",
        "            print(f\"Loading from checkpoint trained on first {ckpt} games.\")\n",
        "            # agent.model.load_state_dict(torch.load(f'{m_checkpoints_folder}{ggg.lower()}_agent_{train_mode}_{train_seed}_{ckpt}_{k}'))\n",
        "        print(f\"Training seed {train_seed} on {n} {ggg} games\")\n",
        "        agent.train()\n",
        "        starttime = time()\n",
        "        nb_episodes = n * k\n",
        "        print(nb_episodes)\n",
        "        avg_moves, avg_norm_scores, trajs = play(agent, path=pth, gamefiles=training_games, nb_episodes=nb_episodes,\n",
        "             verbose=True, seed=train_seed) # Each game will be seen 5 times.\n",
        "        results.append(agent.results)\n",
        "        print(\"Trained in {:.2f} secs\".format(time() - starttime))\n",
        "        if save:\n",
        "            spth = f'{DATA_PTH}{ggg.lower()}_checkpoints/'\n",
        "            if not os.path.exists(spth):\n",
        "                os.makedirs(spth)\n",
        "            torch.save(agent, f\"{spth}{ggg.lower()}_agent_{train_mode}_{train_seed}_{n}_{k}.pt\")\n",
        "            torch.save(agent.model.state_dict(), f\"{spth}{ggg.lower()}_agent_{train_mode}_{train_seed}_{n}_{k}\")\n",
        "            res_df = pd.DataFrame(agent.results).T\n",
        "            res_df.to_pickle(f\"{DATA_PTH}{ggg.lower()}_train_{train_mode}_res_{n}_{k}_{train_seed}.pkl\")\n",
        "            print(f\"Saved {ggg} checkpoint with seed {train_seed}\")\n",
        "        if save_res:\n",
        "            print(\"Saving results.\")\n",
        "            overall_res[ggg.lower()][train_seed] = [avg_moves, avg_norm_scores, trajs]\n",
        "            pd.DataFrame(overall_res).to_pickle(f\"{DATA_PTH}{ggg.lower()}_train_{train_mode}_{train_seed}_{n}_{k}_res.pkl\")\n",
        "    g_results.append(results)\n",
        "    if use_all:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# overall_res = pd.read_pickle(f\"{DATA_PTH}train_res/f_train_w_balanced_qb_nod_6673_240_5_res.pkl\")"
      ],
      "metadata": {
        "id": "Q1FCqi9FJDPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sw6Cz9BOzjG"
      },
      "source": [
        "### Combine genders for dev/test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0GxG-xiBofl"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    m_dev_ulx = get_split_files(m_local_pth, get_split_indices(\"/../content/\", \"m\", \"dev\"))\n",
        "    f_dev_ulx = get_split_files(f_local_pth, get_split_indices(\"/../content/\", \"f\", \"dev\"))\n",
        "    m_test_ulx = get_split_files(m_local_pth, get_split_indices(\"/../content/\", \"m\", \"test\"))\n",
        "    f_test_ulx = get_split_files(f_local_pth, get_split_indices(\"/../content/\", \"f\", \"test\"))\n",
        "except FileNotFoundError as e:\n",
        "    #print(e)\n",
        "    split_pref = f\"{DATA_PTH}\"\n",
        "    m_dev_ulx = get_split_files(m_local_pth, get_split_indices(f\"{split_pref}\", \"m\", \"dev\"))\n",
        "    f_dev_ulx = get_split_files(f_local_pth, get_split_indices(f\"{split_pref}\", \"f\", \"dev\"))\n",
        "    m_test_ulx = get_split_files(m_local_pth, get_split_indices(f\"{split_pref}\", \"m\", \"test\"))\n",
        "    f_test_ulx = get_split_files(f_local_pth, get_split_indices(f\"{split_pref}\", \"f\", \"test\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnw0_vAz9FV5"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    m_train_ulx = get_split_files(m_local_pth,\n",
        "                                get_split_indices(\"/../content/\",\n",
        "                                                    \"m\", \"train\"))\n",
        "    f_train_ulx = get_split_files(f_local_pth,\n",
        "                                get_split_indices(\"/../content/\",\n",
        "                                                    \"f\", \"train\"))\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "    split_pref = f\"{DATA_PTH}\"\n",
        "    try:\n",
        "        m_train_ulx = get_split_files(m_local_pth,\n",
        "                                    get_split_indices(split_pref,\n",
        "                                                        \"m\", \"train\"))\n",
        "        f_train_ulx = get_split_files(f_local_pth,\n",
        "                                    get_split_indices(split_pref,\n",
        "                                                        \"f\", \"train\"))\n",
        "    except:\n",
        "        raise Exception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmlCrmz-hpdV"
      },
      "outputs": [],
      "source": [
        "all_dev = get_combined(m_dev_ulx, f_dev_ulx)\n",
        "all_test = get_combined(m_test_ulx, f_test_ulx)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_train = get_combined(m_train_ulx, f_train_ulx)"
      ],
      "metadata": {
        "id": "lRXUUFXkDo5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NeBFofpeWdW1"
      },
      "outputs": [],
      "source": [
        "#@title save all eval data\n",
        "\n",
        "with open(f\"all_dev_{qtype}_{rewards}{basis}{distractors}.txt\", \"w\") as f:\n",
        "    f.writelines(line + '\\n' for line in all_dev)\n",
        "with open(f\"all_test_{qtype}_{rewards}{basis}{distractors}.txt\", \"w\") as f:\n",
        "    f.writelines(line + '\\n' for line in all_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FNU-amrKe37f"
      },
      "outputs": [],
      "source": [
        "#@title save all train data\n",
        "\n",
        "with open(f\"all_train_{qtype}_{rewards}{basis}{distractors}.txt\", \"w\") as f:\n",
        "    f.writelines(line + '\\n' for line in all_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"f_train_{qtype}_{rewards}{basis}{distractors}_ulx.txt\", \"w\") as f:\n",
        "    f.writelines(line + '\\n' for line in f_train_ulx)"
      ],
      "metadata": {
        "id": "BlSQlqEDJRrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cp /../content/f_train_{{qtype}}_{{rewards}}{{basis}}{{distractors}}_ulx.txt /../content/drive/MyDrive/data/light_data/f_train{{basis}}{{distractors}}_ulx.txt"
      ],
      "metadata": {
        "id": "bFeqEj8MJSDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load all train data\n",
        "\n",
        "try:\n",
        "    with open(f\"{DATA_PTH}all_train_{qtype}_{rewards}{basis}{distractors}.txt\", \"r\") as f:\n",
        "        all_train = f.read().splitlines()\n",
        "except:\n",
        "    %cp {{DATA_PTH}}all_train_{qtype}_{rewards}{{basis}}{{distractors}}.txt /../content/all_train_{qtype}_{rewards}{{basis}}{{distractors}}.txt\n",
        "    with open(f\"{DATA_PTH}all_train_{qtype}_{rewards}{basis}{distractors}.txt\", \"r\") as f:\n",
        "        all_train = f.read().splitlines()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Cr_1aRu9HLBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jYZLj3U7fsd4"
      },
      "outputs": [],
      "source": [
        "#@title load all eval data\n",
        "\n",
        "with open(f\"{DATA_PTH}all_dev_{qtype}_{rewards}{basis}{distractors}.txt\", \"r\") as f:\n",
        "    all_dev = f.read().splitlines()\n",
        "\n",
        "with open(f\"{DATA_PTH}all_test_{qtype}_{rewards}{basis}{distractors}.txt\", \"r\") as f:\n",
        "    all_test = f.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TlFHrdsfDdG"
      },
      "outputs": [],
      "source": [
        "%cp /../content/m_train_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.txt /../content/drive/MyDrive/data/light_data/m_train{{basis}}{{distractors}}.txt\n",
        "%cp /../content/f_train_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.txt /../content/drive/MyDrive/data/light_data/f_train{{basis}}{{distractors}}.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xMGULOscWlQ-"
      },
      "outputs": [],
      "source": [
        "%cp /../content/all_dev_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.txt /../content/drive/MyDrive/data/light_data/all_dev_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.txt\n",
        "%cp /../content/all_test_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.txt /../content/drive/MyDrive/data/light_data/all_test_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cp /../content/all_train_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.txt /../content/drive/MyDrive/data/light_data/all_train_{{qtype}}_{{rewards}}{{basis}}{{distractors}}.txt"
      ],
      "metadata": {
        "id": "ELgiHcinHEAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e46pwORqO7HP"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IMpdn5UJe3K2"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate male, female on combined or swapped data\n",
        "\n",
        "k = 10 #@param {'type': 'integer'}\n",
        "# game_idx = 105 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
        "load_seed = 858 #@param train_seeds = [858, 4386, 1429, 6673, 4368, 7131, 1719, 834, 2968, 7897]\n",
        "load_k = 5 #@param\n",
        "load_n = 240 #@param\n",
        "local = False #@param {'type': 'boolean'}\n",
        "local_eval = False #@param {'type': 'boolean'}\n",
        "bulk = True #@param {'type': 'boolean'}\n",
        "save = True #@param {'type': 'boolean'}\n",
        "eval_all = True #@param {'type': 'boolean'}\n",
        "ext = \"ulx\" #@param [\"ulx\"]\n",
        "split = \"dev\" #@param [\"dev_m\", \"dev_f\", \"test_m\", \"test_f\", \"dev\", \"test\"]\n",
        "single_seed  = False #@param {'type': 'boolean'}\n",
        "seed_from_list = \"4386\" #@param train_seeds = [858, 4386, 1429, 6673, 4368, 7131, 1719, 834, 2968, 7897]\n",
        "seed_from_list = [int(seed_from_list)]\n",
        "load_seed = int(load_seed)\n",
        "train_seeds = train_seeds = [858, 4386, 1429, 6673, 4368, 7131, 1719, 834, 2968, 7897]\n",
        "if not bulk:\n",
        "    if eval_all:\n",
        "        if local:\n",
        "            agent_all = torch.load(f'/../content/all_agent_{qtype}_{rewards}{basis}{distractors}_{load_seed}_{load_n}_{load_k}.pt', map_location=device)\n",
        "        else:\n",
        "            agent_all = torch.load(f'{DATA_PTH}all_checkpoints/all_agent_{qtype}_{rewards}{basis}{distractors}_{load_seed}_{load_n}_{load_k}.pt', map_location=device)\n",
        "        agent_all.test()\n",
        "    if local:\n",
        "        agent = torch.load(f'/../content/m_agent_{qtype}_{rewards}{basis}{distractors}_{load_seed}_{load_n}_{load_k}.pt', map_location=device)\n",
        "        agentB = torch.load(f'/../content/f_agent_{qtype}_{rewards}{basis}{distractors}_{load_seed}_{load_n}_{load_k}.pt', map_location=device)\n",
        "    else:\n",
        "        agent = torch.load(f'{DATA_PTH}m_checkpoints/m_agent_{qtype}_{rewards}{basis}{distractors}_{load_seed}_{load_n}_{load_k}.pt', map_location=device)\n",
        "        agentB = torch.load(f'{DATA_PTH}f_checkpoints/f_agent_{qtype}_{rewards}{basis}{distractors}_{load_seed}_{load_n}_{load_k}.pt', map_location=device)\n",
        "\n",
        "        agent.test()\n",
        "        agentB.test()\n",
        "\n",
        "    if \"dev\" in split:\n",
        "        if split == \"dev_m\":\n",
        "            dtpth = m_dev_ulx\n",
        "        elif split == \"dev_f\":\n",
        "            dtpth = f_dev_ulx\n",
        "        else:\n",
        "            dtpth = all_dev\n",
        "    else:\n",
        "        if split == \"test_m\":\n",
        "            dtpth = m_test_ulx\n",
        "        elif split == \"test_f\":\n",
        "            dtpth = f_test_ulx\n",
        "        else:\n",
        "            dtpth = all_test\n",
        "\n",
        "    if type(dtpth) == str: # dtpth is folder location\n",
        "        if local_eval:\n",
        "            dtpth = all_dev\n",
        "            dtpth = dtpth.replace(f\"{DATA_PTH}quests\", \"/../content\")\n",
        "        games = glob(os.path.join(dtpth, f\"*.{ext}\"))\n",
        "    else: # dtpth already a list of games\n",
        "        games = dtpth\n",
        "    if not \"m\" in split and not \"f\" in split:\n",
        "        indices = list(set(list(map(lambda g: \"_\".join(re.search(r\"test_[0-9]+_[mf]\", g)[0].split(\"_\")[1:]), games))))\n",
        "    else:\n",
        "        indices = list(set(list(map(lambda g: re.search(r\"test_[0-9]+\", g)[0].split(\"_\")[-1], games))))\n",
        "\n",
        "    a_steps = []\n",
        "    a_scores = []\n",
        "    b_steps = []\n",
        "    b_scores = []\n",
        "    a_trajs = []\n",
        "    b_trajs = []\n",
        "    all_steps = []\n",
        "    all_scores = []\n",
        "    all_trajs = []\n",
        "\n",
        "    for ix, game in enumerate(games):\n",
        "        if \"_m.\" in game:\n",
        "            print(\"M\", end=\" \")\n",
        "            game = game.replace(\"_m.\", \".\")\n",
        "            if \"all_dev\" in game:\n",
        "                game = game.replace(\"all_dev\", f\"m_dev_{qtype}_{rewards}\")\n",
        "            elif \"all_test\" in game:\n",
        "                game = game.replace(\"all_test\", f\"m_test_{qtype}_{rewards}\")\n",
        "        elif \"_f.\" in game:\n",
        "            print(\"F\", end=\" \")\n",
        "            game = game.replace(\"_f.\", \".\")\n",
        "            if \"all_dev\" in game:\n",
        "                game = game.replace(\"all_dev\", f\"f_dev_{qtype}_{rewards}\")\n",
        "            elif \"all_test\" in game:\n",
        "                game = game.replace(\"all_test\", f\"f_test_{qtype}_{rewards}\")\n",
        "        if not eval_all:\n",
        "            avg_moves_neuralA, avg_norm_scores_neuralA, trajsA = play(agent, game,\n",
        "                                                                nb_episodes = k,\n",
        "                                                                seed = 123)\n",
        "            avg_moves_neuralB, avg_norm_scores_neuralB, trajsB = play(agentB, game,\n",
        "                                                            nb_episodes = k,\n",
        "                                                            seed = 123)\n",
        "            a_steps.append(np.mean(avg_moves_neuralA))\n",
        "            a_scores.append(np.mean(avg_norm_scores_neuralA))\n",
        "            a_trajs.append(trajsA)\n",
        "            b_steps.append(np.mean(avg_moves_neuralB))\n",
        "            b_scores.append(np.mean(avg_norm_scores_neuralB))\n",
        "            b_trajs.append(trajsB)\n",
        "        else:\n",
        "            avg_moves_neural_all, avg_norm_scores_neural_all, trajs_all = play(agent_all, game,\n",
        "                                                            nb_episodes = k,\n",
        "                                                            seed = 123)\n",
        "            all_steps.append(np.mean(avg_moves_neural_all))\n",
        "            all_scores.append(np.mean(avg_norm_scores_neural_all))\n",
        "            all_trajs.append(trajs_all)\n",
        "else:\n",
        "    load_split = split\n",
        "    print(load_split)\n",
        "    all_res_d = {\"dev\": dict(), \"test\": dict()}\n",
        "    for split in [\"dev\", \"test\"]:\n",
        "        print(split)\n",
        "        all_res_d[split] = {k: dict() for k in train_seeds}\n",
        "        seed_range = train_seeds if not single_seed else seed_from_list\n",
        "        for load_seed in seed_range:\n",
        "            print(load_seed)\n",
        "            if not eval_all:\n",
        "                all_res_d[split][load_seed][\"a_steps\"] = []\n",
        "                all_res_d[split][load_seed][\"a_trajs\"] = []\n",
        "                all_res_d[split][load_seed][\"a_scores\"] = []\n",
        "                all_res_d[split][load_seed][\"b_steps\"] = []\n",
        "                all_res_d[split][load_seed][\"b_trajs\"] = []\n",
        "                all_res_d[split][load_seed][\"b_scores\"] = []\n",
        "            else:\n",
        "                all_res_d[split][load_seed][\"all_steps\"] = []\n",
        "                all_res_d[split][load_seed][\"all_trajs\"] = []\n",
        "                all_res_d[split][load_seed][\"all_scores\"] = []\n",
        "            if eval_all:\n",
        "                if local:\n",
        "                    agent_all = torch.load(f'/../content/all_agent_{qtype}_{rewards}{basis}{distractors}_{load_seed}_{load_n}_{load_k}.pt', map_location=device)\n",
        "                else:\n",
        "                    agent_all = torch.load(f'{DATA_PTH}all_checkpoints/all_agent_{qtype}_{rewards}{basis}{distractors}_{load_seed}_{load_n}_{load_k}.pt', map_location=device)\n",
        "                print(\"Loaded agent.\")\n",
        "                agent_all.test()\n",
        "            else:\n",
        "                if local:\n",
        "                    agent = torch.load(f'/../content/m_agent_{qtype}_{rewards}{basis}{distractors}_{load_seed}_{load_n}_{load_k}.pt', map_location=device)\n",
        "                    agentB = torch.load(f'/../content/f_agent_{qtype}_{rewards}{basis}{distractors}_{load_seed}_{load_n}_{load_k}.pt', map_location=device)\n",
        "                else:\n",
        "                    agent = torch.load(f'{m_checkpoints_folder}m_agent_{qtype}_{rewards}{basis}{distractors}_{load_seed}_{load_n}_{load_k}.pt', map_location=device)\n",
        "                    agentB = torch.load(f'{f_checkpoints_folder}f_agent_{qtype}_{rewards}{basis}{distractors}_{load_seed}_{load_n}_{load_k}.pt', map_location=device)\n",
        "                print(\"Loaded agents.\")\n",
        "                agent.test()\n",
        "                agentB.test()\n",
        "\n",
        "            if \"dev\" in split:\n",
        "                if \"_m\" in load_split:\n",
        "                    dtpth = m_dev_ulx\n",
        "                elif \"_f\" in load_split:\n",
        "                    dtpth = f_dev_ulx\n",
        "                else:\n",
        "                    dtpth = all_dev\n",
        "            else:\n",
        "                if \"_m\" in load_split:\n",
        "                    dtpth = m_test_ulx\n",
        "                elif \"_f\" in load_split:\n",
        "                    dtpth = f_test_ulx\n",
        "                else:\n",
        "                    dtpth = all_test\n",
        "\n",
        "            if type(dtpth) == str: # dtpth is folder location\n",
        "                games = glob(os.path.join(dtpth, f\"*.ulx\"))\n",
        "            else: # dtpth already a list of games\n",
        "                games = dtpth\n",
        "\n",
        "            indices = list(set(list(map(lambda g: re.search(r\"test_[0-9]+\", g)[0].split(\"_\")[-1], games))))\n",
        "            all_res_d[split][load_seed][\"indices\"] = indices\n",
        "\n",
        "            for ix, game in enumerate(games):\n",
        "                if \"_m.\" in game:\n",
        "                    game = game.replace(\"_m.\", \".\")\n",
        "                    if \"all_dev\" in game:\n",
        "                        game = game.replace(\"all_dev\", f\"m_dev_{qtype}_{rewards}\")\n",
        "                    elif \"all_test\" in game:\n",
        "                        game = game.replace(\"all_test\", f\"m_test_{qtype}_{rewards}\")\n",
        "                elif \"_f\" in game:\n",
        "                    game = game.replace(\"_f.\", \".\")\n",
        "                    if \"all_dev\" in game:\n",
        "                        game = game.replace(\"all_dev\", f\"f_dev_{qtype}_{rewards}\")\n",
        "                    elif \"all_test\" in game:\n",
        "                        game = game.replace(\"all_test\", f\"f_test_{qtype}_{rewards}\")\n",
        "                if not eval_all:\n",
        "                    avg_moves_neuralA, avg_norm_scores_neuralA, trajsA = play(agent, game,\n",
        "                                                                        nb_episodes = k,\n",
        "                                                                        seed = 123)\n",
        "                    avg_moves_neuralB, avg_norm_scores_neuralB, trajsB = play(agentB, game,\n",
        "                                                                    nb_episodes = k,\n",
        "                                                                    seed = 123)\n",
        "                    all_res_d[split][load_seed][\"a_steps\"].append(np.mean(avg_moves_neuralA))\n",
        "                    all_res_d[split][load_seed][\"a_scores\"].append(np.mean(avg_norm_scores_neuralA))\n",
        "                    all_res_d[split][load_seed][\"a_trajs\"].append(trajsA)\n",
        "                    all_res_d[split][load_seed][\"b_steps\"].append(np.mean(avg_moves_neuralB))\n",
        "                    all_res_d[split][load_seed][\"b_scores\"].append(np.mean(avg_norm_scores_neuralB))\n",
        "                    all_res_d[split][load_seed][\"b_trajs\"].append(trajsB)\n",
        "                else:\n",
        "                    avg_moves_neural_all, avg_norm_scores_neural_all, trajs_all = play(agent_all, game,\n",
        "                                                                    nb_episodes = k,\n",
        "                                                                    seed = 123)\n",
        "                    all_res_d[split][load_seed][\"all_steps\"].append(np.mean(avg_moves_neural_all))\n",
        "                    all_res_d[split][load_seed][\"all_scores\"].append(np.mean(avg_norm_scores_neural_all))\n",
        "                    all_res_d[split][load_seed][\"all_trajs\"].append(trajs_all)\n",
        "            if save:\n",
        "                if not eval_all:\n",
        "                    a_df = pd.DataFrame(zip(all_res_d[split][load_seed][\"indices\"], all_res_d[split][load_seed][\"a_steps\"], all_res_d[split][load_seed][\"a_scores\"], all_res_d[split][load_seed][\"a_trajs\"]))\n",
        "                    a_df.columns = [\"game_idx\", \"avg_steps\", \"avg_scores\", \"trajs\"]\n",
        "                    b_df = pd.DataFrame(zip(all_res_d[split][load_seed][\"indices\"], all_res_d[split][load_seed][\"b_steps\"], all_res_d[split][load_seed][\"b_scores\"], all_res_d[split][load_seed][\"b_trajs\"]))\n",
        "                    b_df.columns = [\"game_idx\", \"avg_steps\", \"avg_scores\", \"trajs\"]\n",
        "                    if \"_m\" in load_split:\n",
        "                        a_df.to_csv(f\"/../content/drive/MyDrive/data/light_data/m_{qtype}_{rewards}{basis}{distractors}_{split}_{load_seed}_{load_n}_{load_k}_{k}_m.csv\", index=False)\n",
        "                        b_df.to_csv(f\"/../content/drive/MyDrive/data/light_data/f_{qtype}_{rewards}{basis}{distractors}_{split}_{load_seed}_{load_n}_{load_k}_{k}_m.csv\", index=False)\n",
        "                    elif \"_f\" in load_split:\n",
        "                        a_df.to_csv(f\"/../content/drive/MyDrive/data/light_data/m_{qtype}_{rewards}{basis}{distractors}_{split}_{load_seed}_{load_n}_{load_k}_{k}_f.csv\", index=False)\n",
        "                        b_df.to_csv(f\"/../content/drive/MyDrive/data/light_data/f_{qtype}_{rewards}{basis}{distractors}_{split}_{load_seed}_{load_n}_{load_k}_{k}_f.csv\", index=False)\n",
        "                    else:\n",
        "                        a_df.to_csv(f\"/../content/drive/MyDrive/data/light_data/m_{qtype}_{rewards}{basis}{distractors}_{split}_{load_seed}_{load_n}_{load_k}_{k}.csv\", index=False)\n",
        "                        b_df.to_csv(f\"/../content/drive/MyDrive/data/light_data/f_{qtype}_{rewards}{basis}{distractors}_{split}_{load_seed}_{load_n}_{load_k}_{k}.csv\", index=False)\n",
        "                    print(f\"Saved male and female {split} results for seed {load_seed}.\")\n",
        "                else:\n",
        "                    all_df = pd.DataFrame(zip(all_res_d[split][load_seed][\"indices\"],\n",
        "                                              all_res_d[split][load_seed][\"all_steps\"],\n",
        "                                              all_res_d[split][load_seed][\"all_scores\"],\n",
        "                                              all_res_d[split][load_seed][\"all_trajs\"]))\n",
        "                    all_df.columns = [\"game_idx\", \"avg_steps\", \"avg_scores\", \"trajs\"]\n",
        "                    all_df.to_csv(f\"/../content/drive/MyDrive/data/light_data/all_{qtype}_{rewards}{basis}{distractors}_{split}_{load_seed}_{load_n}_{load_k}_{k}.csv\", index=False)\n",
        "                    print(\"Saved all.\")\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OJONoNqZI_Sn"
      },
      "outputs": [],
      "source": [
        "#@title Load eval results\n",
        "test_k = 10 #@param\n",
        "bulk_load = False #@param {'type': 'boolean'}\n",
        "switch_m = False #@param {'type': 'boolean'}\n",
        "switch_f = False #@param {'type': 'boolean'}\n",
        "train_seed = \"1429\" #@param train_seeds = [858, 4386, 1429, 6673, 4368, 7131, 1719, 834, 2968, 7897]\n",
        "train_seed = int(train_seed)\n",
        "train_k = 5 #@param\n",
        "train_n = 240 #@param\n",
        "split = \"test\" #@param [\"dev\", \"test\"]\n",
        "eval_t = \"\" #@param\n",
        "if not bulk_load:\n",
        "    a_pth = f\"{DATA_PTH}m_{qtype}_{rewards}{basis}{distractors}_{split}_{train_seed}_{train_n}_{train_k}_{test_k}\"\n",
        "    b_pth = f\"{DATA_PTH}f_{qtype}_{rewards}{basis}{distractors}_{split}_{train_seed}_{train_n}_{train_k}_{test_k}\"\n",
        "    if switch_m:\n",
        "        a_pth += \"_sm\"\n",
        "        b_pth += \"_sm\"\n",
        "    elif switch_f:\n",
        "        a_pth += \"_sf\"\n",
        "        b_pth += \"_sf\"\n",
        "    a_test_df = pd.read_csv(f\"{a_pth}.csv\")\n",
        "    b_test_df = pd.read_csv(f\"{b_pth}.csv\")\n",
        "else:\n",
        "    all_eval_d = dict()\n",
        "    for split in [\"dev\", \"test\"]:\n",
        "        all_eval_d[split] = {\"m\": dict(), \"f\": dict()}\n",
        "        eval_seeds = [858, 4386, 1429, 6673, 4368, 7131, 1719, 834, 2968, 7897]\n",
        "        if eval_t == \"orig\":\n",
        "            eval_seeds = [858, 4386, 1429, 6673, 4368]\n",
        "        for train_seed in eval_seeds:\n",
        "            if eval_t == \"orig\":\n",
        "                a_pth = f\"{DATA_PTH}eval_results/m_{qtype}_{rewards}{basis}{distractors}_{split}_{train_seed}_{train_n}_{train_k}_{test_k}\"\n",
        "                b_pth = f\"{DATA_PTH}eval_results/f_{qtype}_{rewards}{basis}{distractors}_{split}_{train_seed}_{train_n}_{train_k}_{test_k}\"\n",
        "            else:\n",
        "                a_pth = f\"{DATA_PTH}m_{qtype}_{rewards}{basis}{distractors}_{split}_{train_seed}_{train_n}_{train_k}_{test_k}\"\n",
        "                b_pth = f\"{DATA_PTH}f_{qtype}_{rewards}{basis}{distractors}_{split}_{train_seed}_{train_n}_{train_k}_{test_k}\"\n",
        "            if eval_t == \"orig\":\n",
        "                a_pth += \"_old\"\n",
        "                b_pth += \"_old\"\n",
        "            if switch_m:\n",
        "                a_pth += \"_sm\"\n",
        "                b_pth += \"_sm\"\n",
        "            elif switch_f:\n",
        "                a_pth += \"_sf\"\n",
        "                b_pth += \"_sf\"\n",
        "            try:\n",
        "                all_eval_d[split][\"m\"][train_seed] = pd.read_csv(f\"{a_pth}.csv\")\n",
        "                all_eval_d[split][\"f\"][train_seed] = pd.read_csv(f\"{b_pth}.csv\")\n",
        "            except FileNotFoundError:\n",
        "                if eval_t == \"orig\":\n",
        "                    a_pth = a_pth.replace(\"_old\", \"_orig\")\n",
        "                    b_pth = b_pth.replace(\"_old\", \"_orig\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Significance Tests\n",
        "\n",
        "- [ASO documentation](https://deep-significance.readthedocs.io/en/latest/#id3)"
      ],
      "metadata": {
        "id": "l6f885agDpQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepsig"
      ],
      "metadata": {
        "id": "4j4HaAqDKLSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "from deepsig import aso"
      ],
      "metadata": {
        "id": "JyClnc5WKL9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0dQLKhX4upp"
      },
      "outputs": [],
      "source": [
        "seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "axis = 0 #@param {'type': 'integer'} # avg seed or per seed\n",
        "all_dev_m_scores = pd.DataFrame(all_eval_d[\"dev\"]).m.apply(lambda x: x[\"avg_scores\"]).mean(axis)\n",
        "all_dev_f_scores = pd.DataFrame(all_eval_d[\"dev\"]).f.apply(lambda x: x[\"avg_scores\"]).mean(axis)\n",
        "all_dev_m_steps = pd.DataFrame(all_eval_d[\"dev\"]).m.apply(lambda x: x[\"avg_steps\"]).mean(axis)\n",
        "all_dev_f_steps = pd.DataFrame(all_eval_d[\"dev\"]).f.apply(lambda x: x[\"avg_steps\"]).mean(axis)\n",
        "all_test_m_scores = pd.DataFrame(all_eval_d[\"test\"]).m.apply(lambda x: x[\"avg_scores\"]).mean(axis)\n",
        "all_test_f_scores = pd.DataFrame(all_eval_d[\"test\"]).f.apply(lambda x: x[\"avg_scores\"]).mean(axis)\n",
        "all_test_m_steps = pd.DataFrame(all_eval_d[\"test\"]).m.apply(lambda x: x[\"avg_steps\"]).mean(axis)\n",
        "all_test_f_steps = pd.DataFrame(all_eval_d[\"test\"]).f.apply(lambda x: x[\"avg_steps\"]).mean(axis)\n"
      ],
      "metadata": {
        "id": "Xtx8MuOuLFRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "yIc652Hh_bIW",
        "outputId": "212619c8-118b-4d3d-a9a1-cc3ea36fb9d3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.9942244224422442"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.9481683168316831"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.9946782178217822"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.9513201320132014"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "17.360099009900992"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "30.85168316831683"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "16.44584158415842"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "29.22316831683168"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Avg. game results (scores and steps)\n",
        "display(all_dev_m_scores.mean(), all_dev_f_scores.mean(),\n",
        "        all_test_m_scores.mean(), all_test_f_scores.mean(),\n",
        "        all_dev_m_steps.mean(), all_dev_f_steps.mean(),\n",
        "        all_test_m_steps.mean(), all_test_f_steps.mean())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ASO for avg seed or per seed setting\n",
        "\n",
        "aso(all_dev_m_steps * -1, all_dev_f_steps * -1, seed=42) # lower for steps\n",
        "# aso(all_test_m_steps * -1, all_test_f_steps * -1, seed=42)\n",
        "#aso(all_dev_m_scores, all_dev_f_scores, seed=42) # higher for scores\n",
        "#aso(all_test_m_scores, all_test_f_scores, seed=42) # higher for scores"
      ],
      "metadata": {
        "id": "PlmkB0b0LiUe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wHurRjJ_AyME",
        "nR966zEb2r0u",
        "sj91zCc52pyq",
        "UigO619C_408",
        "gAoK_dIGHcwr",
        "nwQKjkxGB7Sc",
        "2ko_TRJDi-Oa",
        "lc_80zjwuiNz",
        "XzMsFkW8Y4Uv",
        "Whaa_cewJx-o",
        "KuGGgFTxKAcF",
        "1Sw6Cz9BOzjG",
        "e46pwORqO7HP",
        "l6f885agDpQL"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "toc_visible": true,
      "mount_file_id": "1BKtzt0pKJ28yDECVPXYKyyYUBUCtEPVn",
      "authorship_tag": "ABX9TyNdESQVdCPwSTdPMPHAlk+o",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}